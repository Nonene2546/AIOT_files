{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "# !pip install ptflops\n",
    "# from ptflops import get_model_complexity_info\n",
    "# !pip install wget\n",
    "# import wget\n",
    "# !pip install requests gdown\n",
    "# import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter and setting\n",
    "image_size = 224\n",
    "batch_size = 16\n",
    "channel_size = 3\n",
    "lr = 1e-3\n",
    "num_epochs_student = 10\n",
    "num_classes = 31\n",
    "\n",
    "# var init\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transformer = transforms.Compose([\n",
    "    torchvision.models.MobileNet_V3_Small_Weights.IMAGENET1K_V1.transforms(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "DATASET_DIR = '/home/crueang/Chaks/AIOT_project/data/Office-31'\n",
    "\n",
    "amazon_dataset = datasets.ImageFolder(root=f'{DATASET_DIR}/amazon', transform=transformer)\n",
    "dslr_dataset = datasets.ImageFolder(root=f'{DATASET_DIR}/dslr', transform=transformer)\n",
    "webcam_dataset = datasets.ImageFolder(root=f'{DATASET_DIR}/webcam', transform=transformer)\n",
    "\n",
    "classes = webcam_dataset.classes\n",
    "\n",
    "Ds_amazon, Ds_amazon_test = random_split(amazon_dataset, [0.8, 0.2])\n",
    "Ds_dslr, Ds_dslr_test = random_split(dslr_dataset, [0.8, 0.2])\n",
    "Ds_webcam, Ds_webcam_test = random_split(webcam_dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dl_amazon = DataLoader(Ds_amazon, batch_size, shuffle=True, num_workers=2)\n",
    "Dl_dslr = DataLoader(Ds_dslr, batch_size, shuffle=True, num_workers=2)\n",
    "Dl_webcam = DataLoader(Ds_webcam, batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "Dl_amazon_test = DataLoader(Ds_amazon_test, batch_size, shuffle=True, num_workers=2)\n",
    "Dl_dslr_test = DataLoader(Ds_dslr_test, batch_size, shuffle=True, num_workers=2)\n",
    "Dl_webcam_test = DataLoader(Ds_webcam_test, batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(Dl_amazon))\n",
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.suptitle(\"Train vs Validation\")\n",
    "    ax1.plot(history[\"train_src_acc\"], label=\"Train_src_acc\")\n",
    "    ax1.plot(history[\"train_tar_acc\"], label=\"Train_tar_acc\")\n",
    "    ax1.legend()\n",
    "    ax1.set_title(\"Accuracy (Src/Tar)\")\n",
    "\n",
    "    ax2.plot(history[\"train_loss\"], label=\"Train_loss\")\n",
    "    ax2.legend()\n",
    "    ax2.set_title(\"Loss\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationAlignmentLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CorrelationAlignmentLoss, self).__init__()\n",
    "\n",
    "    def forward(self, f_s: torch.Tensor, f_t: torch.Tensor) -> torch.Tensor:\n",
    "        mean_s = f_s.mean(0, keepdim=True)\n",
    "        mean_t = f_t.mean(0, keepdim=True)\n",
    "        cent_s = f_s - mean_s\n",
    "        cent_t = f_t - mean_t\n",
    "        cov_s = torch.mm(cent_s.t(), cent_s) / (len(f_s) - 1)\n",
    "        cov_t = torch.mm(cent_t.t(), cent_t) / (len(f_t) - 1)\n",
    "\n",
    "        mean_diff = (mean_s - mean_t).pow(2).mean()\n",
    "        cov_diff = (cov_s - cov_t).pow(2).mean()\n",
    "\n",
    "        return mean_diff + cov_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_coral = 1.0\n",
    "loss_fn_class = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model, optimizer, Dl_source, Dl_target, Dl_test):\n",
    "    max_batches = min(len(Dl_source), len(Dl_target))\n",
    "    t_0 = time.time()\n",
    "    training_logs_coral = {\"train_loss\": [],  \"train_src_acc\": [], \"train_tar_acc\": [], \"test_acc\": []}\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        train_loss, train_src_correct, train_tar_correct, test_correct = 0, 0, 0, 0\n",
    "        \n",
    "        print(f'epochs {epoch_idx+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
    "        Dl_source_iter = iter(Dl_source)\n",
    "        Dl_target_iter = iter(Dl_target)\n",
    "        Dl_test_iter = iter(Dl_test)\n",
    "\n",
    "        for batch_idx in range(max_batches):\n",
    "        # Actually: (rely on number of data in Dataloader), for i, data in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Training on source domain\n",
    "            X_s, y_s = next(Dl_source_iter)\n",
    "\n",
    "            # Training on target domain\n",
    "            X_t, y_t = next(Dl_target_iter)\n",
    "            \n",
    "            if (Dl_test_iter._num_yielded <= batch_idx):\n",
    "                Dl_test_iter._reset(Dl_test)\n",
    "            X_test, y_test = next(Dl_test_iter)\n",
    "\n",
    "            if X_s.shape[0] != X_t.shape[0]:\n",
    "                min_bs = min(X_s.shape[0], X_t.shape[0])\n",
    "                X_s = X_s[:min_bs]\n",
    "                y_s = y_s[:min_bs]\n",
    "                X_t = X_t[:min_bs]\n",
    "                y_t = y_t[:min_bs]\n",
    "\n",
    "\n",
    "            # GPU processing source and target data\n",
    "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
    "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            class_pred_s = model(X_s)\n",
    "            class_pred_t = model(X_t)\n",
    "            \n",
    "            loss_src = loss_fn_class(class_pred_s, y_s)                 # source classification loss\n",
    "\n",
    "            ################### Regularizer ########################\n",
    "            # Correlation alignment loss\n",
    "            loss_coral = CorrelationAlignmentLoss()(class_pred_s, class_pred_t)\n",
    "            ########################################################\n",
    "\n",
    "            # back-propagation (reversed grad to maximize domain loss)\n",
    "            loss = loss_src + lambda_coral * loss_coral\n",
    "            loss.backward()\n",
    "            # optimization tuning\n",
    "            optimizer.step()\n",
    "\n",
    "            # #output predict from net\n",
    "            with torch.no_grad():\n",
    "                class_prediction_s = model(X_s)\n",
    "                class_prediction_t = model(X_t)\n",
    "                class_prediction_test = model(X_test)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_src_correct += (class_prediction_s.argmax(1) == y_s).float().sum().item()\n",
    "            train_tar_correct += (class_prediction_t.argmax(1) == y_t).float().sum().item()\n",
    "            test_correct += (class_prediction_test.argmax(1) == y_test).float().sum().item()\n",
    "\n",
    "        training_logs_coral[\"train_loss\"].append(train_loss / len(Dl_source))\n",
    "        training_logs_coral[\"train_src_acc\"].append(train_src_correct / len(Dl_source.dataset))#.dataset))\n",
    "        training_logs_coral[\"train_tar_acc\"].append(train_tar_correct / len(Dl_target.dataset))#.dataset))\n",
    "        training_logs_coral[\"test_acc\"].append(test_correct / (batch_size*max_batches))\n",
    "        \n",
    "        if training_logs_coral[\"test_acc\"][-1] == max(training_logs_coral[\"test_acc\"]):\n",
    "            torch.save(model.state_dict(), f'{path_save_cp}best_model.pth')\n",
    "\n",
    "        print(f'Epoch: {epoch_idx+1} || \\\n",
    "        Train_src_acc: {train_src_correct / len(Dl_source.dataset)}, \\\n",
    "        Train_tar_acc: {train_tar_correct / len(Dl_target.dataset)}, \\\n",
    "        Test_acc: {test_correct / (batch_size*max_batches)}, \\\n",
    "        Train_loss: {train_loss / len(Dl_source)}'\n",
    "        )\n",
    "\n",
    "    t_end = time.time()-t_0\n",
    "    print(f\"Time consumption for accelerated CUDA training (device:{device}): {t_end} sec\")\n",
    "\n",
    "    plot_graph(training_logs_coral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/crueang/Chaks/AIOT_project/data/Office-31'\n",
    "HOME = '/home/crueang/Chaks/AIOT_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title FLOPS computation\n",
    "# Code from https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def print_model_param_nums(model=None):\n",
    "    if model == None:\n",
    "        model = torchvision.models.alexnet()\n",
    "    total = sum([param.nelement() if param.requires_grad else 0 for param in model.parameters()])\n",
    "    print('  + Number of params: %.4fM' % (total / 1e6))\n",
    "\n",
    "def count_model_param_flops(model=None, input_res=224, multiply_adds=True, device='cpu'):\n",
    "\n",
    "    prods = {}\n",
    "    def save_hook(name):\n",
    "        def hook_per(self, input, output):\n",
    "            prods[name] = np.prod(input[0].shape)\n",
    "        return hook_per\n",
    "\n",
    "    list_1=[]\n",
    "    def simple_hook(self, input, output):\n",
    "        list_1.append(np.prod(input[0].shape))\n",
    "    list_2={}\n",
    "    def simple_hook2(self, input, output):\n",
    "        list_2['names'] = np.prod(input[0].shape)\n",
    "\n",
    "\n",
    "    list_conv=[]\n",
    "    def conv_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    "\n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        # flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n",
    "\n",
    "        num_weight_params = (self.weight.data != 0).float().sum()\n",
    "        flops = (num_weight_params * (2 if multiply_adds else 1) + bias_ops * output_channels) * output_height * output_width * batch_size\n",
    "\n",
    "        list_conv.append(flops)\n",
    "\n",
    "    list_linear=[]\n",
    "    def linear_hook(self, input, output):\n",
    "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
    "\n",
    "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
    "        bias_ops = self.bias.nelement()\n",
    "\n",
    "        flops = batch_size * (weight_ops + bias_ops)\n",
    "        list_linear.append(flops)\n",
    "\n",
    "    list_bn=[]\n",
    "    def bn_hook(self, input, output):\n",
    "        list_bn.append(input[0].nelement() * 2)\n",
    "\n",
    "    list_relu=[]\n",
    "    def relu_hook(self, input, output):\n",
    "        list_relu.append(input[0].nelement())\n",
    "\n",
    "    list_pooling=[]\n",
    "    def pooling_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        kernel_ops = self.kernel_size * self.kernel_size\n",
    "        bias_ops = 0\n",
    "        params = 0\n",
    "        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n",
    "\n",
    "        list_pooling.append(flops)\n",
    "\n",
    "    list_upsample=[]\n",
    "\n",
    "    # For bilinear upsample\n",
    "    def upsample_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        flops = output_height * output_width * output_channels * batch_size * 12\n",
    "        list_upsample.append(flops)\n",
    "\n",
    "    def foo(net):\n",
    "        childrens = list(net.children())\n",
    "        if not childrens:\n",
    "            if isinstance(net, torch.nn.Conv2d):\n",
    "                net.register_forward_hook(conv_hook)\n",
    "            if isinstance(net, torch.nn.Linear):\n",
    "                net.register_forward_hook(linear_hook)\n",
    "            if isinstance(net, torch.nn.BatchNorm2d):\n",
    "                net.register_forward_hook(bn_hook)\n",
    "            if isinstance(net, torch.nn.ReLU):\n",
    "                net.register_forward_hook(relu_hook)\n",
    "            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
    "                net.register_forward_hook(pooling_hook)\n",
    "            if isinstance(net, torch.nn.Upsample):\n",
    "                net.register_forward_hook(upsample_hook)\n",
    "            return\n",
    "        for c in childrens:\n",
    "            foo(c)\n",
    "\n",
    "    if model == None:\n",
    "        model = torchvision.models.alexnet()\n",
    "    foo(model)\n",
    "    input = Variable(torch.rand(3,input_res,input_res).unsqueeze(0), requires_grad = True).to(device)\n",
    "    out = model(input)\n",
    "\n",
    "\n",
    "    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n",
    "\n",
    "    print('Number of FLOPs: %.6f GFLOPs (%.2f MFLOPs)' % (total_flops / 1e9, total_flops / 1e6))\n",
    "\n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 125,239\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.018660 GFLOPs (18.66 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(18659808.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_fn = torchvision.models.mobilenet_v3_small(weights=None, width_mult=0.25, num_classes=31).to(device)\n",
    "student_fn.load_state_dict(torch.load('/home/crueang/Chaks/AIOT_project/democp/pretrained_mmd26/best_model.pth', weights_only=True, map_location=device))\n",
    "summary(student_fn, input_size=(channel_size, image_size, image_size))\n",
    "count_model_param_flops(model=student_fn.cpu().eval(), input_res=224, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=144, out_features=256, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(student_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer features.0.0: 216 parameters frozen\n",
      "Layer features.0.1: 8 parameters frozen\n",
      "Layer features.0.1: 8 parameters frozen\n",
      "Layer features.1.block.0.0: 72 parameters frozen\n",
      "Layer features.1.block.0.1: 8 parameters frozen\n",
      "Layer features.1.block.0.1: 8 parameters frozen\n",
      "Layer features.1.block.1.fc1: 64 parameters frozen\n",
      "Layer features.1.block.1.fc1: 64 parameters frozen\n",
      "Layer features.1.block.1.fc2: 64 parameters frozen\n",
      "Layer features.1.block.1.fc2: 64 parameters frozen\n",
      "Layer features.1.block.2.0: 64 parameters frozen\n",
      "Layer features.1.block.2.1: 8 parameters frozen\n",
      "Layer features.1.block.2.1: 8 parameters frozen\n",
      "Layer features.2.block.0.0: 192 parameters frozen\n",
      "Layer features.2.block.0.1: 24 parameters frozen\n",
      "Layer features.2.block.0.1: 24 parameters frozen\n",
      "Layer features.2.block.1.0: 216 parameters frozen\n",
      "Layer features.2.block.1.1: 24 parameters frozen\n",
      "Layer features.2.block.1.1: 24 parameters frozen\n",
      "Layer features.2.block.2.0: 192 parameters frozen\n",
      "Layer features.2.block.2.1: 8 parameters frozen\n",
      "Layer features.2.block.2.1: 8 parameters frozen\n",
      "Layer features.3.block.0.0: 192 parameters frozen\n",
      "Layer features.3.block.0.1: 24 parameters frozen\n",
      "Layer features.3.block.0.1: 24 parameters frozen\n",
      "Layer features.3.block.1.0: 216 parameters frozen\n",
      "Layer features.3.block.1.1: 24 parameters frozen\n",
      "Layer features.3.block.1.1: 24 parameters frozen\n",
      "Layer features.3.block.2.0: 192 parameters frozen\n",
      "Layer features.3.block.2.1: 8 parameters frozen\n",
      "Layer features.3.block.2.1: 8 parameters frozen\n",
      "Layer features.4.block.0.0: 192 parameters frozen\n",
      "Layer features.4.block.0.1: 24 parameters frozen\n",
      "Layer features.4.block.0.1: 24 parameters frozen\n",
      "Layer features.4.block.1.0: 600 parameters frozen\n",
      "Layer features.4.block.1.1: 24 parameters frozen\n",
      "Layer features.4.block.1.1: 24 parameters frozen\n",
      "Layer features.4.block.2.fc1: 192 parameters frozen\n",
      "Layer features.4.block.2.fc1: 192 parameters frozen\n",
      "Layer features.4.block.2.fc2: 192 parameters frozen\n",
      "Layer features.4.block.2.fc2: 192 parameters frozen\n",
      "Layer features.4.block.3.0: 384 parameters frozen\n",
      "Layer features.4.block.3.1: 16 parameters frozen\n",
      "Layer features.4.block.3.1: 16 parameters frozen\n",
      "Pruning layer features.5.block.0.0 with 1024 parameters\n",
      "Pruning layer features.5.block.1.0 with 1600 parameters\n",
      "Pruning layer features.5.block.2.fc1 with 1024 parameters\n",
      "Pruning layer features.5.block.2.fc2 with 1024 parameters\n",
      "Pruning layer features.5.block.3.0 with 1024 parameters\n",
      "Pruning layer features.6.block.0.0 with 1024 parameters\n",
      "Pruning layer features.6.block.1.0 with 1600 parameters\n",
      "Pruning layer features.6.block.2.fc1 with 1024 parameters\n",
      "Pruning layer features.6.block.2.fc2 with 1024 parameters\n",
      "Pruning layer features.6.block.3.0 with 1024 parameters\n",
      "Pruning layer features.9.block.0.0 with 1152 parameters\n",
      "Pruning layer features.9.block.1.0 with 1800 parameters\n",
      "Pruning layer features.9.block.2.fc1 with 1728 parameters\n",
      "Pruning layer features.9.block.2.fc2 with 1728 parameters\n",
      "Pruning layer features.9.block.3.0 with 1728 parameters\n",
      "Pruning layer features.10.block.0.0 with 3456 parameters\n",
      "Pruning layer features.10.block.1.0 with 3600 parameters\n",
      "Pruning layer features.10.block.2.fc1 with 5760 parameters\n",
      "Pruning layer features.10.block.2.fc2 with 5760 parameters\n",
      "Pruning layer features.10.block.3.0 with 3456 parameters\n",
      "Pruning layer features.11.block.0.0 with 3456 parameters\n",
      "Pruning layer features.11.block.1.0 with 3600 parameters\n",
      "Pruning layer features.11.block.2.fc1 with 5760 parameters\n",
      "Pruning layer features.11.block.2.fc2 with 5760 parameters\n",
      "Pruning layer features.11.block.3.0 with 3456 parameters\n",
      "Pruning layer features.12.0 with 3456 parameters\n",
      "Pruning layer classifier.0 with 36864 parameters\n",
      "Pruning layer classifier.3 with 7936 parameters\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 3,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.017436 GFLOPs (17.44 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(17436188.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "\n",
    "# f_iterpruned = copy.deepcopy(f)\n",
    "\n",
    "# prune.ln_structured(f_iterpruned[0], name=\"weight\", amount=0.3, n=2, dim=0)\n",
    "\n",
    "# As we can verify, this will zero out all the connections corresponding to\n",
    "# 30% (~2 out of 6) of the channels, while preserving the action of the\n",
    "# previous mask.\n",
    "# for hook in f_iterpruned[0]._forward_pre_hooks.values():\n",
    "#     if hook._tensor_name == \"weight\":  # select out the correct hook\n",
    "#         break\n",
    "def prune_conv_layer(layer, amount=0.5):\n",
    "    \"\"\"\n",
    "    Prune the specified layer by removing 'amount' percentage of channels.\n",
    "    \"\"\"\n",
    "    prune.ln_structured(layer, name=\"weight\", amount=amount, n=1, dim=0)  # Prune filters (channels)\n",
    "\n",
    "\n",
    "# Traverse through the model and prune convolutional layers\n",
    "def apply_pruning(model, amount=0.5, threshold=5000):\n",
    "    flag = 1\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight'):\n",
    "            num_params = module.weight.numel()  # Get number of parameters in the layer\n",
    "            if num_params > threshold:\n",
    "                flag = 0\n",
    "                print(f'Pruning layer {name} with {num_params} parameters')\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "                # prune.remove(module, 'weight')\n",
    "\n",
    "            if flag == 1:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    print(f'Layer {name}: {num_params} parameters frozen')\n",
    "\n",
    "\n",
    "# Apply pruning to the model\n",
    "apply_pruning(student_fn, amount=0.2, threshold=1000)\n",
    "\n",
    "def remove_pruning_masks(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            try:\n",
    "                prune.remove(module, 'weight')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# remove_pruning_masks(student_fn)\n",
    "\n",
    "# student_fn = torchvision.models.mobilenet_v3_small(weights=None, width_mult=0.25, num_classes=31).to(device)\n",
    "summary(student_fn.to(device), input_size=(channel_size, image_size, image_size))\n",
    "count_model_param_flops(model=student_fn.cpu().eval(), input_res=224, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for module in student_fn.modules():\n",
    "#     if hasattr(module, '_forward_hooks'):\n",
    "#         print(module._forward_hooks)\n",
    "#         module._forward_hooks.clear()\n",
    "# torch.save(student_fn, 'model.pth')\n",
    "# loaded = torch.load('/home/crueang/Chaks/AIOT_project/cp/mbv3_red_coral_A2D_prune_pre_structed_1k_freeze_before_0_2/model.pth')\n",
    "# summary(loaded.to(device), input_size=(channel_size, image_size, image_size))\n",
    "# count_model_param_flops(model=loaded.cpu().eval(), input_res=224, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=144, out_features=256, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      ")\n",
      "features Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "        (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "        (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      ")\n",
      "features.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.0.0 Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "features.0.1 BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.0.2 Hardswish()\n",
      "features.1 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "      (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.1.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "    (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.1.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "  (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.1.block.0.0 Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "features.1.block.0.1 BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.1.block.0.2 ReLU(inplace=True)\n",
      "features.1.block.1 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.1.block.1.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.1.block.1.fc1 Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.1.block.1.fc2 Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.1.block.1.activation ReLU()\n",
      "features.1.block.1.scale_activation Hardsigmoid()\n",
      "features.1.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.1.block.2.0 Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.1.block.2.1 BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.2 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.2.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.2.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.2.block.0.0 Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.2.block.0.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.2.block.0.2 ReLU(inplace=True)\n",
      "features.2.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.2.block.1.0 Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "features.2.block.1.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.2.block.1.2 ReLU(inplace=True)\n",
      "features.2.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.2.block.2.0 Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.2.block.2.1 BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.3 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.3.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.3.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.3.block.0.0 Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.3.block.0.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.3.block.0.2 ReLU(inplace=True)\n",
      "features.3.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.3.block.1.0 Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "features.3.block.1.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.3.block.1.2 ReLU(inplace=True)\n",
      "features.3.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.3.block.2.0 Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.3.block.2.1 BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.4 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.4.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.4.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.4.block.0.0 Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.4.block.0.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.4.block.0.2 Hardswish()\n",
      "features.4.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.4.block.1.0 Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
      "features.4.block.1.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.4.block.1.2 Hardswish()\n",
      "features.4.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.4.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.4.block.2.fc1 Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.4.block.2.fc2 Conv2d(8, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.4.block.2.activation ReLU()\n",
      "features.4.block.2.scale_activation Hardsigmoid()\n",
      "features.4.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.4.block.3.0 Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.4.block.3.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.5 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.5.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "    (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.5.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.5.block.0.0 Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.5.block.0.1 BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.5.block.0.2 Hardswish()\n",
      "features.5.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "  (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.5.block.1.0 Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "features.5.block.1.1 BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.5.block.1.2 Hardswish()\n",
      "features.5.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.5.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.5.block.2.fc1 Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.5.block.2.fc2 Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.5.block.2.activation ReLU()\n",
      "features.5.block.2.scale_activation Hardsigmoid()\n",
      "features.5.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.5.block.3.0 Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.5.block.3.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.6 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.6.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "    (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.6.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.6.block.0.0 Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.6.block.0.1 BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.6.block.0.2 Hardswish()\n",
      "features.6.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "  (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.6.block.1.0 Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "features.6.block.1.1 BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.6.block.1.2 Hardswish()\n",
      "features.6.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.6.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.6.block.2.fc1 Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.6.block.2.fc2 Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.6.block.2.activation ReLU()\n",
      "features.6.block.2.scale_activation Hardsigmoid()\n",
      "features.6.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.6.block.3.0 Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.6.block.3.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.7 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.7.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.7.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.7.block.0.0 Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.7.block.0.1 BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.7.block.0.2 Hardswish()\n",
      "features.7.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "  (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.7.block.1.0 Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "features.7.block.1.1 BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.7.block.1.2 Hardswish()\n",
      "features.7.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.7.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.7.block.2.fc1 Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.7.block.2.fc2 Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.7.block.2.activation ReLU()\n",
      "features.7.block.2.scale_activation Hardsigmoid()\n",
      "features.7.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.7.block.3.0 Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.7.block.3.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.8 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.8.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
      "    (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.8.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.8.block.0.0 Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.8.block.0.1 BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.8.block.0.2 Hardswish()\n",
      "features.8.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
      "  (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.8.block.1.0 Conv2d(40, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
      "features.8.block.1.1 BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.8.block.1.2 Hardswish()\n",
      "features.8.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.8.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.8.block.2.fc1 Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.8.block.2.fc2 Conv2d(16, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.8.block.2.activation ReLU()\n",
      "features.8.block.2.scale_activation Hardsigmoid()\n",
      "features.8.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.8.block.3.0 Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.8.block.3.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.9 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.9.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "    (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.9.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.9.block.0.0 Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.9.block.0.1 BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.9.block.0.2 Hardswish()\n",
      "features.9.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "  (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.9.block.1.0 Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "features.9.block.1.1 BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.9.block.1.2 Hardswish()\n",
      "features.9.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.9.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.9.block.2.fc1 Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.9.block.2.fc2 Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.9.block.2.activation ReLU()\n",
      "features.9.block.2.scale_activation Hardsigmoid()\n",
      "features.9.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.9.block.3.0 Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.9.block.3.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.10 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.10.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.10.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.10.block.0.0 Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.10.block.0.1 BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.10.block.0.2 Hardswish()\n",
      "features.10.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "  (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.10.block.1.0 Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "features.10.block.1.1 BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.10.block.1.2 Hardswish()\n",
      "features.10.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.10.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.10.block.2.fc1 Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.10.block.2.fc2 Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.10.block.2.activation ReLU()\n",
      "features.10.block.2.scale_activation Hardsigmoid()\n",
      "features.10.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.10.block.3.0 Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.10.block.3.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.11 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.11.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "    (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.11.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.11.block.0.0 Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.11.block.0.1 BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.11.block.0.2 Hardswish()\n",
      "features.11.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "  (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.11.block.1.0 Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "features.11.block.1.1 BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.11.block.1.2 Hardswish()\n",
      "features.11.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.11.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.11.block.2.fc1 Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.11.block.2.fc2 Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.11.block.2.activation ReLU()\n",
      "features.11.block.2.scale_activation Hardsigmoid()\n",
      "features.11.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.11.block.3.0 Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.11.block.3.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.12 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.12.0 Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.12.1 BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.12.2 Hardswish()\n",
      "avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "classifier Sequential(\n",
      "  (0): Linear(in_features=144, out_features=256, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=256, out_features=31, bias=True)\n",
      ")\n",
      "classifier.0 Linear(in_features=144, out_features=256, bias=True)\n",
      "classifier.1 Hardswish()\n",
      "classifier.2 Dropout(p=0.2, inplace=True)\n",
      "classifier.3 Linear(in_features=256, out_features=31, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in student_fn.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/crueang/Chaks/AIOT_project/data/Office-31'\n",
    "HOME = '/home/crueang/Chaks/AIOT_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 16\n",
    "channel_size = 3\n",
    "lr = 1e-3\n",
    "num_epochs = 50\n",
    "num_classes = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "def train(model, opt, loss_fn, train_loader, test_loader, epochs=10, checkpoint_path=None, device='cpu'):\n",
    "    training_logs = {\"train_loss\": [], \"validate_loss\": [], \"train_acc\": [], \"validate_acc\": []}\n",
    "    epoch_number = 0\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    if checkpoint_path:\n",
    "      if os.path.exists(checkpoint_path + 'training_logs.pth'):\n",
    "        training_logs = torch.load(checkpoint_path + 'training_logs.pth', weights_only=True)\n",
    "        epoch_number = len(training_logs['train_loss'])\n",
    "        best_test_loss = min(best_test_loss, min(training_logs['validate_loss']))\n",
    "        \n",
    "      if os.path.exists(checkpoint_path + 'model.pth'):\n",
    "        model = torch.load(checkpoint_path + 'model.pth', weights_only=False, map_location=device)\n",
    "\n",
    "      if os.path.exists(checkpoint_path + 'opt.pth'):\n",
    "        opt.load_state_dict(torch.load(checkpoint_path + 'opt.pth', weights_only=True, map_location=device))\n",
    "\n",
    "\n",
    "    for i in range(epoch_number):\n",
    "        print(f\"Epochs {i+1}\".ljust(10), end='')\n",
    "        for k, v in training_logs.items():\n",
    "            print(f\"{k}: {v[i]:.5f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "    print(\"Training on\", device)\n",
    "    for epoch in range(epoch_number, epochs):\n",
    "        model = model.to(device)\n",
    "        train_loss, train_correct = 0, 0\n",
    "        model.train()\n",
    "        train_bar = tqdm(train_loader, desc=f'Training Epoch [{epoch+1}/{epochs}]', unit='batch')\n",
    "        for images, label in train_bar:\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            opt.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == label).float().sum().item()\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        avg_train_acc = train_correct / len(train_loader.dataset)\n",
    "        print(f'\\n\\tTrain loss: {avg_train_loss}')\n",
    "        print(f'\\tTrain acc: {avg_train_acc}')\n",
    "        training_logs[\"train_loss\"].append(avg_train_loss)\n",
    "        training_logs[\"train_acc\"].append(avg_train_acc)\n",
    "\n",
    "        test_loss, test_correct = 0, 0\n",
    "        model.eval()\n",
    "        test_bar = tqdm(test_loader,desc='Testing',unit='batch')\n",
    "        with torch.no_grad():\n",
    "          for images, label in test_bar:\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, label)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += (outputs.argmax(1) == label).float().sum().item()\n",
    "            \n",
    "        avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "        avg_test_acc = test_correct / len(test_loader.dataset)\n",
    "        print(f'\\tTest loss: {avg_test_loss}')\n",
    "        print(f'\\tTest acc: {avg_test_acc}')\n",
    "        training_logs[\"validate_loss\"].append(avg_test_loss)\n",
    "        training_logs[\"validate_acc\"].append(avg_test_acc)\n",
    "\n",
    "        if checkpoint_path:\n",
    "            for module in model.modules():\n",
    "                if hasattr(module, '_forward_hooks'):\n",
    "                    module._forward_hooks.clear()\n",
    "            torch.save(model.cpu().eval(), checkpoint_path + \"model.pth\")\n",
    "            torch.save(opt.state_dict(), checkpoint_path + \"opt.pth\")\n",
    "            torch.save(training_logs, checkpoint_path + 'training_logs.pth')\n",
    "            if best_test_loss > avg_test_loss:\n",
    "               torch.save(model, checkpoint_path + \"best_model.pth\")\n",
    "               best_test_loss = avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP = '/home/crueang/Chaks/AIOT_project/democp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_fn = torch.load(f'{HOME}/cp/mbv3_red_coral_A2D_prune_pre1/model.pth', weights_only=False, map_location=device)\n",
    "\n",
    "optimizer = torch.optim.Adam(student_fn.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(amazon_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dslr_dataset, batch_size=batch_size, shuffle=False)\n",
    "checkpoint_path = f'{CP}/prune/prune1/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "student_fn = student_fn.to(device)\n",
    "summary(student_fn, (3, 224, 224))\n",
    "count_model_param_flops(model=student_fn.cpu().eval(), input_res=224, multiply_adds=True)\n",
    "train(student_fn, optimizer, criterion, train_loader, test_loader, 15, checkpoint_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 3,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.017436 GFLOPs (17.44 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(17436188.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.load(f'/home/crueang/Chaks/AIOT_project/democp/prune/prune1/best_model.pth', weights_only=False, map_location=device)\n",
    "summary(loaded_model, (3, 224, 224))\n",
    "count_model_param_flops(model=loaded_model.cpu().eval(), input_res=224, multiply_adds=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandDataset(Dataset):\n",
    "    def __init__(self, original_dataset, required_size):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.required_size = required_size\n",
    "        self.original_size = len(original_dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.required_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Select a random index from the original dataset to duplicate\n",
    "        original_idx = idx % self.original_size  # Loop around if idx exceeds original size\n",
    "        return self.original_dataset[original_idx]\n",
    "\n",
    "expanded_dataset = ExpandDataset(dslr_dataset, 2817)\n",
    "Dl_dslr = DataLoader(expanded_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_coral(model, optimizer, Dl_source, Dl_target, Dl_test, checkpoint_path):\n",
    "    max_batches = min(len(Dl_source), len(Dl_target))\n",
    "    t_0 = time.time()\n",
    "    training_logs_coral = {\"train_loss\": [],  \"train_src_acc\": [], \"train_tar_acc\": []}\n",
    "    epoch_number = 0\n",
    "    \n",
    "    if checkpoint_path:\n",
    "        if os.path.exists(checkpoint_path + 'model.pth'):\n",
    "            model = torch.load(checkpoint_path + 'model.pth', weights_only=False, map_location=device)\n",
    "\n",
    "        if os.path.exists(checkpoint_path + 'opt.pth'):\n",
    "            optimizer.load_state_dict(torch.load(checkpoint_path + 'opt.pth', weights_only=True, map_location=device))\n",
    "\n",
    "        if os.path.exists(checkpoint_path + 'training_logs.pth'):\n",
    "            training_logs_coral = torch.load(checkpoint_path + 'training_logs.pth', weights_only=True)\n",
    "            epoch_number = len(training_logs_coral['train_loss'])\n",
    "    \n",
    "    for i in range(epoch_number):\n",
    "        print(f\"Epochs {i+1}\".ljust(10), end='')\n",
    "        for k, v in training_logs_coral.items():\n",
    "            print(f\"{k}: {v[i]:.5f}\", end=\" \")\n",
    "        print()\n",
    "        \n",
    "    for epoch_idx in range(epoch_number, num_epochs):\n",
    "        train_loss, train_src_correct, train_tar_correct, test_correct = 0, 0, 0, 0\n",
    "        \n",
    "        print(f'epochs {epoch_idx+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
    "        Dl_source_iter = iter(Dl_source)\n",
    "        Dl_target_iter = iter(Dl_target)\n",
    "        # Dl_test_iter = iter(Dl_test)\n",
    "\n",
    "        for batch_idx in range(max_batches):\n",
    "        # Actually: (rely on number of data in Dataloader), for i, data in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Training on source domain\n",
    "            X_s, y_s = next(Dl_source_iter)\n",
    "\n",
    "            # Training on target domain\n",
    "            X_t, y_t = next(Dl_target_iter)\n",
    "            \n",
    "            # if (Dl_test_iter._num_yielded <= batch_idx):\n",
    "            #     Dl_test_iter._reset(Dl_test)\n",
    "            # X_test, y_test = next(Dl_test_iter)\n",
    "\n",
    "            if X_s.shape[0] != X_t.shape[0]:\n",
    "                min_bs = min(X_s.shape[0], X_t.shape[0])\n",
    "                X_s = X_s[:min_bs]\n",
    "                y_s = y_s[:min_bs]\n",
    "                X_t = X_t[:min_bs]\n",
    "                y_t = y_t[:min_bs]\n",
    "\n",
    "\n",
    "            # GPU processing source and target data\n",
    "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
    "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
    "            # X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            class_pred_s = model(X_s)\n",
    "            class_pred_t = model(X_t)\n",
    "            \n",
    "            loss_src = loss_fn_class(class_pred_s, y_s)                 # source classification loss\n",
    "\n",
    "            ################### Regularizer ########################\n",
    "            # Correlation alignment loss\n",
    "            loss_coral = CorrelationAlignmentLoss()(class_pred_s, class_pred_t)\n",
    "            ########################################################\n",
    "\n",
    "            # back-propagation (reversed grad to maximize domain loss)\n",
    "            loss = loss_src + lambda_coral * loss_coral\n",
    "            loss.backward()\n",
    "            # optimization tuning\n",
    "            optimizer.step()\n",
    "\n",
    "            # #output predict from net\n",
    "            with torch.no_grad():\n",
    "                class_prediction_s = model(X_s)\n",
    "                class_prediction_t = model(X_t)\n",
    "            #     class_prediction_test = model(X_test)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_src_correct += (class_prediction_s.argmax(1) == y_s).float().sum().item()\n",
    "            train_tar_correct += (class_prediction_t.argmax(1) == y_t).float().sum().item()\n",
    "            # test_correct += (class_prediction_test.argmax(1) == y_test).float().sum().item()\n",
    "\n",
    "        training_logs_coral[\"train_loss\"].append(train_loss / len(Dl_source))\n",
    "        training_logs_coral[\"train_src_acc\"].append(train_src_correct / len(Dl_source.dataset))#.dataset))\n",
    "        training_logs_coral[\"train_tar_acc\"].append(train_tar_correct / len(Dl_target.dataset))#.dataset))\n",
    "        # training_logs_coral[\"test_acc\"].append(test_correct / (batch_size*max_batches))\n",
    "        \n",
    "        if training_logs_coral[\"train_tar_acc\"][-1] == max(training_logs_coral[\"train_tar_acc\"]):\n",
    "            torch.save(model, f'{checkpoint_path}best_model.pth')\n",
    "            \n",
    "        torch.save(model, checkpoint_path + \"model.pth\")\n",
    "        torch.save(optimizer.state_dict(), checkpoint_path + \"opt.pth\")\n",
    "        torch.save(training_logs_coral, checkpoint_path + 'training_logs.pth')\n",
    "\n",
    "        print(f'Epoch: {epoch_idx+1} || \\\n",
    "        Train_src_acc: {train_src_correct / len(Dl_source.dataset)}, \\\n",
    "        Train_tar_acc: {train_tar_correct / len(Dl_target.dataset)}, \\\n",
    "        Train_loss: {train_loss / len(Dl_source)}'\n",
    "        )\n",
    "\n",
    "    t_end = time.time()-t_0\n",
    "    print(f\"Time consumption for accelerated CUDA training (device:{device}): {t_end} sec\")\n",
    "\n",
    "    plot_graph(training_logs_coral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.0.weight, requires_grad: False\n",
      "Layer: features.0.1.weight, requires_grad: False\n",
      "Layer: features.0.1.bias, requires_grad: False\n",
      "Layer: features.1.block.0.0.weight, requires_grad: False\n",
      "Layer: features.1.block.0.1.weight, requires_grad: False\n",
      "Layer: features.1.block.0.1.bias, requires_grad: False\n",
      "Layer: features.1.block.1.fc1.weight, requires_grad: False\n",
      "Layer: features.1.block.1.fc1.bias, requires_grad: False\n",
      "Layer: features.1.block.1.fc2.weight, requires_grad: False\n",
      "Layer: features.1.block.1.fc2.bias, requires_grad: False\n",
      "Layer: features.1.block.2.0.weight, requires_grad: False\n",
      "Layer: features.1.block.2.1.weight, requires_grad: False\n",
      "Layer: features.1.block.2.1.bias, requires_grad: False\n",
      "Layer: features.2.block.0.0.weight, requires_grad: False\n",
      "Layer: features.2.block.0.1.weight, requires_grad: False\n",
      "Layer: features.2.block.0.1.bias, requires_grad: False\n",
      "Layer: features.2.block.1.0.weight, requires_grad: False\n",
      "Layer: features.2.block.1.1.weight, requires_grad: False\n",
      "Layer: features.2.block.1.1.bias, requires_grad: False\n",
      "Layer: features.2.block.2.0.weight, requires_grad: False\n",
      "Layer: features.2.block.2.1.weight, requires_grad: False\n",
      "Layer: features.2.block.2.1.bias, requires_grad: False\n",
      "Layer: features.3.block.0.0.weight, requires_grad: False\n",
      "Layer: features.3.block.0.1.weight, requires_grad: False\n",
      "Layer: features.3.block.0.1.bias, requires_grad: False\n",
      "Layer: features.3.block.1.0.weight, requires_grad: False\n",
      "Layer: features.3.block.1.1.weight, requires_grad: False\n",
      "Layer: features.3.block.1.1.bias, requires_grad: False\n",
      "Layer: features.3.block.2.0.weight, requires_grad: False\n",
      "Layer: features.3.block.2.1.weight, requires_grad: False\n",
      "Layer: features.3.block.2.1.bias, requires_grad: False\n",
      "Layer: features.4.block.0.0.weight, requires_grad: False\n",
      "Layer: features.4.block.0.1.weight, requires_grad: False\n",
      "Layer: features.4.block.0.1.bias, requires_grad: False\n",
      "Layer: features.4.block.1.0.weight, requires_grad: False\n",
      "Layer: features.4.block.1.1.weight, requires_grad: False\n",
      "Layer: features.4.block.1.1.bias, requires_grad: False\n",
      "Layer: features.4.block.2.fc1.weight, requires_grad: False\n",
      "Layer: features.4.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.4.block.2.fc2.weight, requires_grad: False\n",
      "Layer: features.4.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.4.block.3.0.weight, requires_grad: False\n",
      "Layer: features.4.block.3.1.weight, requires_grad: False\n",
      "Layer: features.4.block.3.1.bias, requires_grad: False\n",
      "Layer: features.5.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.0.1.weight, requires_grad: False\n",
      "Layer: features.5.block.0.1.bias, requires_grad: False\n",
      "Layer: features.5.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.1.1.weight, requires_grad: False\n",
      "Layer: features.5.block.1.1.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.3.1.weight, requires_grad: False\n",
      "Layer: features.5.block.3.1.bias, requires_grad: False\n",
      "Layer: features.6.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.0.1.weight, requires_grad: False\n",
      "Layer: features.6.block.0.1.bias, requires_grad: False\n",
      "Layer: features.6.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.1.1.weight, requires_grad: False\n",
      "Layer: features.6.block.1.1.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.3.1.weight, requires_grad: False\n",
      "Layer: features.6.block.3.1.bias, requires_grad: False\n",
      "Layer: features.7.block.0.0.weight, requires_grad: False\n",
      "Layer: features.7.block.0.1.weight, requires_grad: False\n",
      "Layer: features.7.block.0.1.bias, requires_grad: False\n",
      "Layer: features.7.block.1.0.weight, requires_grad: False\n",
      "Layer: features.7.block.1.1.weight, requires_grad: False\n",
      "Layer: features.7.block.1.1.bias, requires_grad: False\n",
      "Layer: features.7.block.2.fc1.weight, requires_grad: False\n",
      "Layer: features.7.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.7.block.2.fc2.weight, requires_grad: False\n",
      "Layer: features.7.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.7.block.3.0.weight, requires_grad: False\n",
      "Layer: features.7.block.3.1.weight, requires_grad: False\n",
      "Layer: features.7.block.3.1.bias, requires_grad: False\n",
      "Layer: features.8.block.0.0.weight, requires_grad: False\n",
      "Layer: features.8.block.0.1.weight, requires_grad: False\n",
      "Layer: features.8.block.0.1.bias, requires_grad: False\n",
      "Layer: features.8.block.1.0.weight, requires_grad: False\n",
      "Layer: features.8.block.1.1.weight, requires_grad: False\n",
      "Layer: features.8.block.1.1.bias, requires_grad: False\n",
      "Layer: features.8.block.2.fc1.weight, requires_grad: False\n",
      "Layer: features.8.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.8.block.2.fc2.weight, requires_grad: False\n",
      "Layer: features.8.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.8.block.3.0.weight, requires_grad: False\n",
      "Layer: features.8.block.3.1.weight, requires_grad: False\n",
      "Layer: features.8.block.3.1.bias, requires_grad: False\n",
      "Layer: features.9.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.0.1.weight, requires_grad: False\n",
      "Layer: features.9.block.0.1.bias, requires_grad: False\n",
      "Layer: features.9.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.1.1.weight, requires_grad: False\n",
      "Layer: features.9.block.1.1.bias, requires_grad: False\n",
      "Layer: features.9.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.9.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.9.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.3.1.weight, requires_grad: False\n",
      "Layer: features.9.block.3.1.bias, requires_grad: False\n",
      "Layer: features.10.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.0.1.weight, requires_grad: False\n",
      "Layer: features.10.block.0.1.bias, requires_grad: False\n",
      "Layer: features.10.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.1.1.weight, requires_grad: False\n",
      "Layer: features.10.block.1.1.bias, requires_grad: False\n",
      "Layer: features.10.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.10.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.10.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.3.1.weight, requires_grad: False\n",
      "Layer: features.10.block.3.1.bias, requires_grad: False\n",
      "Layer: features.11.block.0.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.0.1.weight, requires_grad: True\n",
      "Layer: features.11.block.0.1.bias, requires_grad: True\n",
      "Layer: features.11.block.1.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.1.1.weight, requires_grad: True\n",
      "Layer: features.11.block.1.1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.3.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.3.1.weight, requires_grad: True\n",
      "Layer: features.11.block.3.1.bias, requires_grad: True\n",
      "Layer: features.12.0.weight_orig, requires_grad: True\n",
      "Layer: features.12.1.weight, requires_grad: True\n",
      "Layer: features.12.1.bias, requires_grad: True\n",
      "Layer: classifier.0.bias, requires_grad: True\n",
      "Layer: classifier.0.weight_orig, requires_grad: True\n",
      "Layer: classifier.3.bias, requires_grad: True\n",
      "Layer: classifier.3.weight_orig, requires_grad: True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 71,671\n",
      "Non-trainable params: 53,568\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.017436 GFLOPs (17.44 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(17436188.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_fn = torch.load(f'{HOME}/cp/mbv3_red_coral_A2D_prune_pre_structed_1k_freeze_before_0_2/model.pth', weights_only=False, map_location=device)\n",
    "student_fn = student_fn.to(device)\n",
    "for name, param in student_fn.named_parameters():\n",
    "    if 'features' in name and int(name.split('.')[1]) < 11:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "    print(f'Layer: {name}, requires_grad: {param.requires_grad}')\n",
    "summary(student_fn, (3, 224, 224))\n",
    "count_model_param_flops(model=student_fn.cpu().eval(), input_res=224, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0001 / 0050\n",
      "============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 ||         Train_src_acc: 0.883318544809228,         Train_tar_acc: 0.6861909833155839,         Train_loss: 43.85652248910133\n",
      "epochs 0002 / 0050\n",
      "============\n",
      "Epoch: 2 ||         Train_src_acc: 0.7466725820763088,         Train_tar_acc: 0.5931842385516507,         Train_loss: 2.9682574221428406\n",
      "epochs 0003 / 0050\n",
      "============\n",
      "Epoch: 3 ||         Train_src_acc: 0.7795031055900621,         Train_tar_acc: 0.6205182818601349,         Train_loss: 2.2532382704687457\n",
      "epochs 0004 / 0050\n",
      "============\n",
      "Epoch: 4 ||         Train_src_acc: 0.8371783496007098,         Train_tar_acc: 0.645367412140575,         Train_loss: 2.0152244178961354\n",
      "epochs 0005 / 0050\n",
      "============\n",
      "Epoch: 5 ||         Train_src_acc: 0.8837622005323869,         Train_tar_acc: 0.6755413560525382,         Train_loss: 1.845996601361755\n",
      "epochs 0006 / 0050\n",
      "============\n",
      "Epoch: 6 ||         Train_src_acc: 0.9228039041703638,         Train_tar_acc: 0.6929357472488463,         Train_loss: 1.6934238756802065\n",
      "epochs 0007 / 0050\n",
      "============\n",
      "Epoch: 7 ||         Train_src_acc: 0.9445430346051464,         Train_tar_acc: 0.708910188143415,         Train_loss: 1.5673329204532271\n",
      "epochs 0008 / 0050\n",
      "============\n",
      "Epoch: 8 ||         Train_src_acc: 0.9622892635314996,         Train_tar_acc: 0.726659566915158,         Train_loss: 1.4762953638185001\n",
      "epochs 0009 / 0050\n",
      "============\n",
      "Epoch: 9 ||         Train_src_acc: 0.968944099378882,         Train_tar_acc: 0.7355342563010294,         Train_loss: 1.4001127092550831\n",
      "epochs 0010 / 0050\n",
      "============\n",
      "Epoch: 10 ||         Train_src_acc: 0.9773735581188997,         Train_tar_acc: 0.7465388711395101,         Train_loss: 1.3262078347781026\n",
      "epochs 0011 / 0050\n",
      "============\n",
      "Epoch: 11 ||         Train_src_acc: 0.9831410825199645,         Train_tar_acc: 0.7539936102236422,         Train_loss: 1.2653304503319112\n",
      "epochs 0012 / 0050\n",
      "============\n",
      "Epoch: 12 ||         Train_src_acc: 0.9880212954747116,         Train_tar_acc: 0.7557685481008165,         Train_loss: 1.221483939928366\n",
      "epochs 0013 / 0050\n",
      "============\n",
      "Epoch: 13 ||         Train_src_acc: 0.9897959183673469,         Train_tar_acc: 0.7653532126375577,         Train_loss: 1.1654152324859133\n",
      "epochs 0014 / 0050\n",
      "============\n",
      "Epoch: 14 ||         Train_src_acc: 0.9924578527062999,         Train_tar_acc: 0.7667731629392971,         Train_loss: 1.1247522792917617\n",
      "epochs 0015 / 0050\n",
      "============\n",
      "Epoch: 15 ||         Train_src_acc: 0.9937888198757764,         Train_tar_acc: 0.7749378771742989,         Train_loss: 1.0831408390762112\n",
      "epochs 0016 / 0050\n",
      "============\n",
      "Epoch: 16 ||         Train_src_acc: 0.9946761313220941,         Train_tar_acc: 0.7738729144479943,         Train_loss: 1.049395220499512\n",
      "epochs 0017 / 0050\n",
      "============\n",
      "Epoch: 17 ||         Train_src_acc: 0.9964507542147294,         Train_tar_acc: 0.7767128150514732,         Train_loss: 1.0196865653315335\n",
      "epochs 0018 / 0050\n",
      "============\n",
      "Epoch: 18 ||         Train_src_acc: 0.9977817213842058,         Train_tar_acc: 0.7784877529286475,         Train_loss: 0.9736492232228002\n",
      "epochs 0019 / 0050\n",
      "============\n",
      "Epoch: 19 ||         Train_src_acc: 0.9977817213842058,         Train_tar_acc: 0.7809726659566916,         Train_loss: 0.953600304769286\n",
      "epochs 0020 / 0050\n",
      "============\n",
      "Epoch: 20 ||         Train_src_acc: 0.9977817213842058,         Train_tar_acc: 0.7781327653532126,         Train_loss: 0.9228336603928965\n",
      "epochs 0021 / 0050\n",
      "============\n",
      "Epoch: 21 ||         Train_src_acc: 0.9991126885536823,         Train_tar_acc: 0.7813276535321264,         Train_loss: 0.9011203976387673\n",
      "epochs 0022 / 0050\n",
      "============\n",
      "Epoch: 22 ||         Train_src_acc: 0.9991126885536823,         Train_tar_acc: 0.7799077032303869,         Train_loss: 0.870098888874054\n",
      "epochs 0023 / 0050\n",
      "============\n",
      "Epoch: 23 ||         Train_src_acc: 0.9995563442768411,         Train_tar_acc: 0.7799077032303869,         Train_loss: 0.8516123252557525\n",
      "epochs 0024 / 0050\n",
      "============\n",
      "Epoch: 24 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7767128150514732,         Train_loss: 0.8168859105583623\n",
      "epochs 0025 / 0050\n",
      "============\n",
      "Epoch: 25 ||         Train_src_acc: 0.9995563442768411,         Train_tar_acc: 0.774582889598864,         Train_loss: 0.7947932693129736\n",
      "epochs 0026 / 0050\n",
      "============\n",
      "Epoch: 26 ||         Train_src_acc: 0.9995563442768411,         Train_tar_acc: 0.7760028399006035,         Train_loss: 0.7685005584507124\n",
      "epochs 0027 / 0050\n",
      "============\n",
      "Epoch: 27 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.774582889598864,         Train_loss: 0.7612661300821507\n",
      "epochs 0028 / 0050\n",
      "============\n",
      "Epoch: 28 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7756478523251686,         Train_loss: 0.7350356384372034\n",
      "epochs 0029 / 0050\n",
      "============\n",
      "Epoch: 29 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7724529641462549,         Train_loss: 0.7122184997754739\n",
      "epochs 0030 / 0050\n",
      "============\n",
      "Epoch: 30 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7767128150514732,         Train_loss: 0.6969753112776059\n",
      "epochs 0031 / 0050\n",
      "============\n",
      "Epoch: 31 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7689030883919062,         Train_loss: 0.6705881224033681\n",
      "epochs 0032 / 0050\n",
      "============\n",
      "Epoch: 32 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7724529641462549,         Train_loss: 0.6524254459861323\n",
      "epochs 0033 / 0050\n",
      "============\n",
      "Epoch: 33 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7692580759673412,         Train_loss: 0.6412652794350969\n",
      "epochs 0034 / 0050\n",
      "============\n",
      "Epoch: 34 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7681931132410366,         Train_loss: 0.6248066063891066\n",
      "epochs 0035 / 0050\n",
      "============\n",
      "Epoch: 35 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7618033368832091,         Train_loss: 0.6021441777124472\n",
      "epochs 0036 / 0050\n",
      "============\n",
      "Epoch: 36 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7657082002129926,         Train_loss: 0.5914342779639765\n",
      "epochs 0037 / 0050\n",
      "============\n",
      "Epoch: 37 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7596734114306,         Train_loss: 0.573116162144546\n",
      "epochs 0038 / 0050\n",
      "============\n",
      "Epoch: 38 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7600283990060348,         Train_loss: 0.5534230052156651\n",
      "epochs 0039 / 0050\n",
      "============\n",
      "Epoch: 39 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7500887468938587,         Train_loss: 0.5489908403961371\n",
      "epochs 0040 / 0050\n",
      "============\n",
      "Epoch: 40 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7422790202342918,         Train_loss: 0.5351554814805376\n",
      "epochs 0041 / 0050\n",
      "============\n",
      "Epoch: 41 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7525736599219027,         Train_loss: 0.5212755809861718\n",
      "epochs 0042 / 0050\n",
      "============\n",
      "Epoch: 42 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7376641817536386,         Train_loss: 0.511661373131664\n",
      "epochs 0043 / 0050\n",
      "============\n",
      "Epoch: 43 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7380191693290735,         Train_loss: 0.49298384299514986\n",
      "epochs 0044 / 0050\n",
      "============\n",
      "Epoch: 44 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7312744053958111,         Train_loss: 0.48665969612750604\n",
      "epochs 0045 / 0050\n",
      "============\n",
      "Epoch: 45 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7302094426695066,         Train_loss: 0.4781450090256143\n",
      "epochs 0046 / 0050\n",
      "============\n",
      "Epoch: 46 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7341143059992901,         Train_loss: 0.4710823415864444\n",
      "epochs 0047 / 0050\n",
      "============\n",
      "Epoch: 47 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.713880014199503,         Train_loss: 0.46305773528755134\n",
      "epochs 0048 / 0050\n",
      "============\n",
      "Epoch: 48 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7273695420660277,         Train_loss: 0.4459932841730456\n",
      "epochs 0049 / 0050\n",
      "============\n",
      "Epoch: 49 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7206247781327654,         Train_loss: 0.44019257176852394\n",
      "epochs 0050 / 0050\n",
      "============\n",
      "Epoch: 50 ||         Train_src_acc: 1.0,         Train_tar_acc: 0.7145899893503728,         Train_loss: 0.43222273748817175\n",
      "Time consumption for accelerated CUDA training (device:cuda): 1673.8482253551483 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = torchvision.models.mobilenet_v3_small(weights=None, width_mult=0.25, num_classes=31).to(device)\n",
    "# model.load_state_dict(torch.load(f'{HOME}/cp/mbv3_small_reduce/best_model.pth', weights_only=True))\n",
    "# student_fn = torch.load('/home/crueang/Chaks/AIOT_project/cp/mbv3_red_coral_A2D_prune_pre_structed_1k_freeze_before/best_model.pth', weights_only=False, map_location=device)\n",
    "# student_fn = student_fn.to(device)\n",
    "# summary(student_fn, (3, 224, 224))\n",
    "path_save_cp = f'{HOME}/cp/mbv3_red_coral_A2D_prune_struct_1k_freeze_before_5_0_2/'\n",
    "optimizer = optim.Adam(student_fn.parameters(), 1e-3)\n",
    "\n",
    "for module in student_fn.modules():\n",
    "    if hasattr(module, '_forward_hooks'):\n",
    "        module._forward_hooks.clear()\n",
    "\n",
    "student_fn = student_fn.to(device)\n",
    "\n",
    "if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n",
    "\n",
    "train_coral(student_fn, optimizer, Dl_amazon, Dl_dslr, Dl_dslr_test, path_save_cp)\n",
    "\n",
    "\n",
    "torch.save(student_fn, path_save_cp+'coral_A-D.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.0.weight, requires_grad: False\n",
      "Layer: features.0.1.weight, requires_grad: False\n",
      "Layer: features.0.1.bias, requires_grad: False\n",
      "Layer: features.1.block.0.0.weight, requires_grad: False\n",
      "Layer: features.1.block.0.1.weight, requires_grad: False\n",
      "Layer: features.1.block.0.1.bias, requires_grad: False\n",
      "Layer: features.1.block.1.fc1.weight, requires_grad: False\n",
      "Layer: features.1.block.1.fc1.bias, requires_grad: False\n",
      "Layer: features.1.block.1.fc2.weight, requires_grad: False\n",
      "Layer: features.1.block.1.fc2.bias, requires_grad: False\n",
      "Layer: features.1.block.2.0.weight, requires_grad: False\n",
      "Layer: features.1.block.2.1.weight, requires_grad: False\n",
      "Layer: features.1.block.2.1.bias, requires_grad: False\n",
      "Layer: features.2.block.0.0.weight, requires_grad: False\n",
      "Layer: features.2.block.0.1.weight, requires_grad: False\n",
      "Layer: features.2.block.0.1.bias, requires_grad: False\n",
      "Layer: features.2.block.1.0.weight, requires_grad: False\n",
      "Layer: features.2.block.1.1.weight, requires_grad: False\n",
      "Layer: features.2.block.1.1.bias, requires_grad: False\n",
      "Layer: features.2.block.2.0.weight, requires_grad: False\n",
      "Layer: features.2.block.2.1.weight, requires_grad: False\n",
      "Layer: features.2.block.2.1.bias, requires_grad: False\n",
      "Layer: features.3.block.0.0.weight, requires_grad: False\n",
      "Layer: features.3.block.0.1.weight, requires_grad: False\n",
      "Layer: features.3.block.0.1.bias, requires_grad: False\n",
      "Layer: features.3.block.1.0.weight, requires_grad: False\n",
      "Layer: features.3.block.1.1.weight, requires_grad: False\n",
      "Layer: features.3.block.1.1.bias, requires_grad: False\n",
      "Layer: features.3.block.2.0.weight, requires_grad: False\n",
      "Layer: features.3.block.2.1.weight, requires_grad: False\n",
      "Layer: features.3.block.2.1.bias, requires_grad: False\n",
      "Layer: features.4.block.0.0.weight, requires_grad: False\n",
      "Layer: features.4.block.0.1.weight, requires_grad: False\n",
      "Layer: features.4.block.0.1.bias, requires_grad: False\n",
      "Layer: features.4.block.1.0.weight, requires_grad: False\n",
      "Layer: features.4.block.1.1.weight, requires_grad: False\n",
      "Layer: features.4.block.1.1.bias, requires_grad: False\n",
      "Layer: features.4.block.2.fc1.weight, requires_grad: False\n",
      "Layer: features.4.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.4.block.2.fc2.weight, requires_grad: False\n",
      "Layer: features.4.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.4.block.3.0.weight, requires_grad: False\n",
      "Layer: features.4.block.3.1.weight, requires_grad: False\n",
      "Layer: features.4.block.3.1.bias, requires_grad: False\n",
      "Layer: features.5.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.0.1.weight, requires_grad: False\n",
      "Layer: features.5.block.0.1.bias, requires_grad: False\n",
      "Layer: features.5.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.1.1.weight, requires_grad: False\n",
      "Layer: features.5.block.1.1.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.3.1.weight, requires_grad: False\n",
      "Layer: features.5.block.3.1.bias, requires_grad: False\n",
      "Layer: features.6.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.0.1.weight, requires_grad: False\n",
      "Layer: features.6.block.0.1.bias, requires_grad: False\n",
      "Layer: features.6.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.1.1.weight, requires_grad: False\n",
      "Layer: features.6.block.1.1.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.3.1.weight, requires_grad: False\n",
      "Layer: features.6.block.3.1.bias, requires_grad: False\n",
      "Layer: features.7.block.0.0.weight, requires_grad: False\n",
      "Layer: features.7.block.0.1.weight, requires_grad: False\n",
      "Layer: features.7.block.0.1.bias, requires_grad: False\n",
      "Layer: features.7.block.1.0.weight, requires_grad: False\n",
      "Layer: features.7.block.1.1.weight, requires_grad: False\n",
      "Layer: features.7.block.1.1.bias, requires_grad: False\n",
      "Layer: features.7.block.2.fc1.weight, requires_grad: False\n",
      "Layer: features.7.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.7.block.2.fc2.weight, requires_grad: False\n",
      "Layer: features.7.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.7.block.3.0.weight, requires_grad: False\n",
      "Layer: features.7.block.3.1.weight, requires_grad: False\n",
      "Layer: features.7.block.3.1.bias, requires_grad: False\n",
      "Layer: features.8.block.0.0.weight, requires_grad: False\n",
      "Layer: features.8.block.0.1.weight, requires_grad: False\n",
      "Layer: features.8.block.0.1.bias, requires_grad: False\n",
      "Layer: features.8.block.1.0.weight, requires_grad: False\n",
      "Layer: features.8.block.1.1.weight, requires_grad: False\n",
      "Layer: features.8.block.1.1.bias, requires_grad: False\n",
      "Layer: features.8.block.2.fc1.weight, requires_grad: False\n",
      "Layer: features.8.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.8.block.2.fc2.weight, requires_grad: False\n",
      "Layer: features.8.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.8.block.3.0.weight, requires_grad: False\n",
      "Layer: features.8.block.3.1.weight, requires_grad: False\n",
      "Layer: features.8.block.3.1.bias, requires_grad: False\n",
      "Layer: features.9.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.0.1.weight, requires_grad: False\n",
      "Layer: features.9.block.0.1.bias, requires_grad: False\n",
      "Layer: features.9.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.1.1.weight, requires_grad: False\n",
      "Layer: features.9.block.1.1.bias, requires_grad: False\n",
      "Layer: features.9.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.9.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.9.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.9.block.3.1.weight, requires_grad: False\n",
      "Layer: features.9.block.3.1.bias, requires_grad: False\n",
      "Layer: features.10.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.0.1.weight, requires_grad: False\n",
      "Layer: features.10.block.0.1.bias, requires_grad: False\n",
      "Layer: features.10.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.1.1.weight, requires_grad: False\n",
      "Layer: features.10.block.1.1.bias, requires_grad: False\n",
      "Layer: features.10.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.10.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.10.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.10.block.3.1.weight, requires_grad: False\n",
      "Layer: features.10.block.3.1.bias, requires_grad: False\n",
      "Layer: features.11.block.0.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.0.1.weight, requires_grad: True\n",
      "Layer: features.11.block.0.1.bias, requires_grad: True\n",
      "Layer: features.11.block.1.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.1.1.weight, requires_grad: True\n",
      "Layer: features.11.block.1.1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.3.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.3.1.weight, requires_grad: True\n",
      "Layer: features.11.block.3.1.bias, requires_grad: True\n",
      "Layer: features.12.0.weight_orig, requires_grad: True\n",
      "Layer: features.12.1.weight, requires_grad: True\n",
      "Layer: features.12.1.bias, requires_grad: True\n",
      "Layer: classifier.0.bias, requires_grad: True\n",
      "Layer: classifier.0.weight_orig, requires_grad: True\n",
      "Layer: classifier.3.bias, requires_grad: True\n",
      "Layer: classifier.3.weight_orig, requires_grad: True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 71,671\n",
      "Non-trainable params: 53,568\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.017436 GFLOPs (17.44 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(17436188.)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_fn = torch.load(f'{HOME}/cp/mbv3_red_coral_A2D_prune_pre_structed_1k_freeze_before_0_2/model.pth', weights_only=False, map_location=device)\n",
    "student_fn = student_fn.to(device)\n",
    "for name, param in student_fn.named_parameters():\n",
    "    if 'features' in name and int(name.split('.')[1]) < 11:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "    print(f'Layer: {name}, requires_grad: {param.requires_grad}')\n",
    "summary(student_fn, (3, 224, 224))\n",
    "count_model_param_flops(model=student_fn.cpu().eval(), input_res=224, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student_fn.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_set, test_set = random_split(dslr_dataset, [0.2, 0.8])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "checkpoint_path = f'{HOME}/cp/pruning/pre_structed_1k_freeze_features_0_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/50]:   0%|          | 0/7 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/50]: 100%|| 7/7 [00:01<00:00,  5.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTrain loss: 0.04858837932348251\n",
      "\tTrain acc: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 25/25 [00:03<00:00,  6.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.0063480936943193025\n",
      "\tTest acc: 0.9748743718592965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [2/50]: 100%|| 7/7 [00:00<00:00,  7.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTrain loss: 0.020093266256153583\n",
      "\tTrain acc: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 25/25 [00:03<00:00,  6.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.006772229425730792\n",
      "\tTest acc: 0.964824120603015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3/50]: 100%|| 7/7 [00:00<00:00,  7.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTrain loss: 0.020579888373613357\n",
      "\tTrain acc: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  64%|   | 16/25 [00:02<00:01,  6.37batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(student_fn, optimizer, criterion, train_loader, test_loader, \u001b[38;5;241m50\u001b[39m, checkpoint_path, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[49], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, opt, loss_fn, train_loader, test_loader, epochs, checkpoint_path, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m test_bar \u001b[38;5;241m=\u001b[39m tqdm(test_loader,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting\u001b[39m\u001b[38;5;124m'\u001b[39m,unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 54\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m images, label \u001b[38;5;129;01min\u001b[39;00m test_bar:\n\u001b[1;32m     55\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(student_fn, optimizer, criterion, train_loader, test_loader, 50, checkpoint_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 3,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.017436 GFLOPs (17.44 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(17436188.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(f'{HOME}/cp/mbv3_red_coral_A2D_prune_pre_structed_1k_freeze_before_0_2/model.pth', weights_only=False, map_location=device)\n",
    "summary(model, (3, 224, 224))\n",
    "count_model_param_flops(model=model.cpu().eval(), input_res=224, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/crueang/Chaks/AIOT_project/data/Office-31'\n",
    "HOME = '/home/crueang/Chaks/AIOT_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREVIOUS_MODEL = f'{CP}/prune/global_prune/v1/best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 3,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.016487 GFLOPs (16.49 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16486875.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_fn_globpruned = torch.load(PREVIOUS_MODEL, weights_only=False, map_location=device)\n",
    "summary(student_fn_globpruned, input_size=(channel_size, image_size, image_size))\n",
    "count_model_param_flops(model=student_fn_globpruned.cpu().eval(), input_res=image_size, multiply_adds=True)\n",
    "# f = student_fn_globpruned.features\n",
    "# C = student_fn_globpruned.classifier\n",
    "# print(f)\n",
    "# for name, module in model.named_modules():\n",
    "#     if hasattr(module, 'weight'):\n",
    "#         num_params = module.weight.numel()  # Get number of parameters in the layer\n",
    "#         if num_params > 1000:\n",
    "#             print(name, num_params)\n",
    "        #     flag = 0\n",
    "        #     print(f'Pruning layer {name} with {num_params} parameters')\n",
    "        #     prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "        #     # prune.remove(module, 'weight')\n",
    "\n",
    "        # if flag == 1:\n",
    "        #     for param in module.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        #         print(f'Layer {name}: {num_params} parameters frozen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 3,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.016298 GFLOPs (16.30 MFLOPs)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "So far, we only looked at what is usually referred to as local pruning, i.e.,\n",
    "the practice of pruning tensors in a model one by one, by comparing the\n",
    "statistics (weight magnitude, activation, gradient, etc.) of each entry\n",
    "exclusively to the other entries in that tensor. However, a common and perhaps\n",
    "more powerful technique is to prune the model all at once, by removing\n",
    "(for example) the lowest 20% of connections across the whole model, instead of\n",
    "removing the lowest 20% of connections in each layer. This is likely to result\n",
    "in different pruning percentages per layer. Lets see how to do that using\n",
    "global_unstructured from 'torch.nn.utils.prune.'\n",
    "\"\"\"\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "student_fn_globpruned = torch.load(PREVIOUS_MODEL, weights_only=False, map_location=device)\n",
    "f = student_fn_globpruned.features\n",
    "C = student_fn_globpruned.classifier\n",
    "f_allpruned = copy.deepcopy(f)\n",
    "C_allpruned = copy.deepcopy(C)\n",
    "\n",
    "parameters_to_prune = (\n",
    "    \n",
    "    # (f_allpruned[4].block[0][0], 'weight'),\n",
    "    # (f_allpruned[4].block[1][0], 'weight'),\n",
    "    # (f_allpruned[4].block[2].fc1, 'weight'),\n",
    "    # (f_allpruned[4].block[2].fc2, 'weight'),\n",
    "    # (f_allpruned[4].block[3][0], 'weight'),\n",
    "    \n",
    "    # (f_allpruned[5].block[0][0], 'weight'),\n",
    "    # (f_allpruned[5].block[1][0], 'weight'),\n",
    "    # (f_allpruned[5].block[2].fc1, 'weight'),\n",
    "    # (f_allpruned[5].block[2].fc2, 'weight'),\n",
    "    # (f_allpruned[5].block[3][0], 'weight'),\n",
    "    \n",
    "    (f_allpruned[6].block[0][0], 'weight'),\n",
    "    (f_allpruned[6].block[1][0], 'weight'),\n",
    "    (f_allpruned[6].block[2].fc1, 'weight'),\n",
    "    (f_allpruned[6].block[2].fc2, 'weight'),\n",
    "    (f_allpruned[6].block[3][0], 'weight'),\n",
    "    \n",
    "    (f_allpruned[7].block[0][0], 'weight'),\n",
    "    (f_allpruned[7].block[1][0], 'weight'),\n",
    "    (f_allpruned[7].block[2].fc1, 'weight'),\n",
    "    (f_allpruned[7].block[2].fc2, 'weight'),\n",
    "    (f_allpruned[7].block[3][0], 'weight'),\n",
    "    \n",
    "    # (f_allpruned[8].block[0][0], 'weight'),\n",
    "    # (f_allpruned[8].block[1][0], 'weight'),\n",
    "    # (f_allpruned[8].block[2].fc1, 'weight'),\n",
    "    # (f_allpruned[8].block[2].fc2, 'weight'),\n",
    "    # (f_allpruned[8].block[3][0], 'weight'),\n",
    "    \n",
    "    # (f_allpruned[9].block[0][0], 'weight'),\n",
    "    # (f_allpruned[9].block[1][0], 'weight'),\n",
    "    # (f_allpruned[9].block[2].fc1, 'weight'),\n",
    "    # (f_allpruned[9].block[2].fc2, 'weight'),\n",
    "    # (f_allpruned[9].block[3][0], 'weight'),\n",
    "    \n",
    "    # (f_allpruned[10].block[0][0], 'weight'),\n",
    "    # (f_allpruned[10].block[1][0], 'weight'),\n",
    "    # (f_allpruned[10].block[2].fc1, 'weight'),\n",
    "    # (f_allpruned[10].block[2].fc2, 'weight'),\n",
    "    # (f_allpruned[10].block[3][0], 'weight'),\n",
    "    \n",
    "    # (f_allpruned[11].block[0][0], 'weight'),\n",
    "    # (f_allpruned[11].block[1][0], 'weight'),\n",
    "    # (f_allpruned[11].block[2].fc1, 'weight'),\n",
    "    # (f_allpruned[11].block[2].fc2, 'weight'),\n",
    "    # (f_allpruned[11].block[3][0], 'weight'),\n",
    "\n",
    "    # (C_allpruned[0], 'weight'),\n",
    "    # (C_allpruned[3], 'weight'),\n",
    ")\n",
    "\n",
    "# Pruning the same setting\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.10,\n",
    ")\n",
    "\n",
    "student_fn_globpruned = torch.load(PREVIOUS_MODEL, weights_only=False, map_location=device)\n",
    "student_fn_globpruned.features = f_allpruned\n",
    "student_fn_globpruned.classifier = C_allpruned\n",
    "student_fn_globpruned.to(device)\n",
    "summary(student_fn_globpruned, input_size=(channel_size, image_size, image_size))\n",
    "count_model_param_flops(model=student_fn_globpruned.cpu().eval(), input_res=224, multiply_adds=True)\n",
    "for module in student_fn_globpruned.modules():\n",
    "    if hasattr(module, '_forward_hooks'):\n",
    "        module._forward_hooks.clear()\n",
    "PATH_SAVE = '/home/crueang/Chaks/AIOT_project/cp/pruning/global_prune/v1/pruned.pth'\n",
    "torch.save(student_fn_globpruned.cpu().eval(), PATH_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 3,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.016298 GFLOPs (16.30 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16298461.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(PATH_SAVE, weights_only=False, map_location=device)\n",
    "summary(model, input_size=(channel_size, image_size, image_size))\n",
    "count_model_param_flops(model=model.cpu().eval(), input_res=image_size, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 3,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.016298 GFLOPs (16.30 MFLOPs)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(PATH_SAVE, weights_only=False, map_location=device)\n",
    "summary(model, input_size=(channel_size, image_size, image_size))\n",
    "count_model_param_flops(model=model.cpu().eval(), input_res=image_size, multiply_adds=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(amazon_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dslr_dataset, batch_size=batch_size, shuffle=False)\n",
    "checkpoint_path = f'{CP}/prune/global_prune/v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.0.weight, requires_grad: False\n",
      "Layer: features.0.1.weight, requires_grad: False\n",
      "Layer: features.0.1.bias, requires_grad: False\n",
      "Layer: features.1.block.0.0.weight, requires_grad: False\n",
      "Layer: features.1.block.0.1.weight, requires_grad: False\n",
      "Layer: features.1.block.0.1.bias, requires_grad: False\n",
      "Layer: features.1.block.1.fc1.weight, requires_grad: False\n",
      "Layer: features.1.block.1.fc1.bias, requires_grad: False\n",
      "Layer: features.1.block.1.fc2.weight, requires_grad: False\n",
      "Layer: features.1.block.1.fc2.bias, requires_grad: False\n",
      "Layer: features.1.block.2.0.weight, requires_grad: False\n",
      "Layer: features.1.block.2.1.weight, requires_grad: False\n",
      "Layer: features.1.block.2.1.bias, requires_grad: False\n",
      "Layer: features.2.block.0.0.weight, requires_grad: False\n",
      "Layer: features.2.block.0.1.weight, requires_grad: False\n",
      "Layer: features.2.block.0.1.bias, requires_grad: False\n",
      "Layer: features.2.block.1.0.weight, requires_grad: False\n",
      "Layer: features.2.block.1.1.weight, requires_grad: False\n",
      "Layer: features.2.block.1.1.bias, requires_grad: False\n",
      "Layer: features.2.block.2.0.weight, requires_grad: False\n",
      "Layer: features.2.block.2.1.weight, requires_grad: False\n",
      "Layer: features.2.block.2.1.bias, requires_grad: False\n",
      "Layer: features.3.block.0.0.weight, requires_grad: False\n",
      "Layer: features.3.block.0.1.weight, requires_grad: False\n",
      "Layer: features.3.block.0.1.bias, requires_grad: False\n",
      "Layer: features.3.block.1.0.weight, requires_grad: False\n",
      "Layer: features.3.block.1.1.weight, requires_grad: False\n",
      "Layer: features.3.block.1.1.bias, requires_grad: False\n",
      "Layer: features.3.block.2.0.weight, requires_grad: False\n",
      "Layer: features.3.block.2.1.weight, requires_grad: False\n",
      "Layer: features.3.block.2.1.bias, requires_grad: False\n",
      "Layer: features.4.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.4.block.0.1.weight, requires_grad: False\n",
      "Layer: features.4.block.0.1.bias, requires_grad: False\n",
      "Layer: features.4.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.4.block.1.1.weight, requires_grad: False\n",
      "Layer: features.4.block.1.1.bias, requires_grad: False\n",
      "Layer: features.4.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.4.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.4.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.4.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.4.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.4.block.3.1.weight, requires_grad: False\n",
      "Layer: features.4.block.3.1.bias, requires_grad: False\n",
      "Layer: features.5.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.0.1.weight, requires_grad: False\n",
      "Layer: features.5.block.0.1.bias, requires_grad: False\n",
      "Layer: features.5.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.1.1.weight, requires_grad: False\n",
      "Layer: features.5.block.1.1.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.5.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.5.block.3.1.weight, requires_grad: False\n",
      "Layer: features.5.block.3.1.bias, requires_grad: False\n",
      "Layer: features.6.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.0.1.weight, requires_grad: False\n",
      "Layer: features.6.block.0.1.bias, requires_grad: False\n",
      "Layer: features.6.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.1.1.weight, requires_grad: False\n",
      "Layer: features.6.block.1.1.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.6.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.6.block.3.1.weight, requires_grad: False\n",
      "Layer: features.6.block.3.1.bias, requires_grad: False\n",
      "Layer: features.7.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.7.block.0.1.weight, requires_grad: False\n",
      "Layer: features.7.block.0.1.bias, requires_grad: False\n",
      "Layer: features.7.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.7.block.1.1.weight, requires_grad: False\n",
      "Layer: features.7.block.1.1.bias, requires_grad: False\n",
      "Layer: features.7.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.7.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.7.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.7.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.7.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.7.block.3.1.weight, requires_grad: False\n",
      "Layer: features.7.block.3.1.bias, requires_grad: False\n",
      "Layer: features.8.block.0.0.weight_orig, requires_grad: False\n",
      "Layer: features.8.block.0.1.weight, requires_grad: False\n",
      "Layer: features.8.block.0.1.bias, requires_grad: False\n",
      "Layer: features.8.block.1.0.weight_orig, requires_grad: False\n",
      "Layer: features.8.block.1.1.weight, requires_grad: False\n",
      "Layer: features.8.block.1.1.bias, requires_grad: False\n",
      "Layer: features.8.block.2.fc1.bias, requires_grad: False\n",
      "Layer: features.8.block.2.fc1.weight_orig, requires_grad: False\n",
      "Layer: features.8.block.2.fc2.bias, requires_grad: False\n",
      "Layer: features.8.block.2.fc2.weight_orig, requires_grad: False\n",
      "Layer: features.8.block.3.0.weight_orig, requires_grad: False\n",
      "Layer: features.8.block.3.1.weight, requires_grad: False\n",
      "Layer: features.8.block.3.1.bias, requires_grad: False\n",
      "Layer: features.9.block.0.0.weight_orig, requires_grad: True\n",
      "Layer: features.9.block.0.1.weight, requires_grad: True\n",
      "Layer: features.9.block.0.1.bias, requires_grad: True\n",
      "Layer: features.9.block.1.0.weight_orig, requires_grad: True\n",
      "Layer: features.9.block.1.1.weight, requires_grad: True\n",
      "Layer: features.9.block.1.1.bias, requires_grad: True\n",
      "Layer: features.9.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.9.block.2.fc1.weight_orig, requires_grad: True\n",
      "Layer: features.9.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.9.block.2.fc2.weight_orig, requires_grad: True\n",
      "Layer: features.9.block.3.0.weight_orig, requires_grad: True\n",
      "Layer: features.9.block.3.1.weight, requires_grad: True\n",
      "Layer: features.9.block.3.1.bias, requires_grad: True\n",
      "Layer: features.10.block.0.0.weight_orig, requires_grad: True\n",
      "Layer: features.10.block.0.1.weight, requires_grad: True\n",
      "Layer: features.10.block.0.1.bias, requires_grad: True\n",
      "Layer: features.10.block.1.0.weight_orig, requires_grad: True\n",
      "Layer: features.10.block.1.1.weight, requires_grad: True\n",
      "Layer: features.10.block.1.1.bias, requires_grad: True\n",
      "Layer: features.10.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.10.block.2.fc1.weight_orig, requires_grad: True\n",
      "Layer: features.10.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.10.block.2.fc2.weight_orig, requires_grad: True\n",
      "Layer: features.10.block.3.0.weight_orig, requires_grad: True\n",
      "Layer: features.10.block.3.1.weight, requires_grad: True\n",
      "Layer: features.10.block.3.1.bias, requires_grad: True\n",
      "Layer: features.11.block.0.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.0.1.weight, requires_grad: True\n",
      "Layer: features.11.block.0.1.bias, requires_grad: True\n",
      "Layer: features.11.block.1.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.1.1.weight, requires_grad: True\n",
      "Layer: features.11.block.1.1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.3.0.weight_orig, requires_grad: True\n",
      "Layer: features.11.block.3.1.weight, requires_grad: True\n",
      "Layer: features.11.block.3.1.bias, requires_grad: True\n",
      "Layer: features.12.0.weight_orig, requires_grad: True\n",
      "Layer: features.12.1.weight, requires_grad: True\n",
      "Layer: features.12.1.bias, requires_grad: True\n",
      "Layer: classifier.0.bias, requires_grad: True\n",
      "Layer: classifier.0.weight_orig, requires_grad: True\n",
      "Layer: classifier.3.bias, requires_grad: True\n",
      "Layer: classifier.3.weight_orig, requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'features' in name and int(name.split('.')[1]) < 9:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "    print(f'Layer: {name}, requires_grad: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/50]: 100%|| 177/177 [00:11<00:00, 15.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTrain loss: 0.01164968422193174\n",
      "\tTrain acc: 0.952076677316294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 32/32 [00:05<00:00,  6.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.009467706091182191\n",
      "\tTest acc: 0.9477911646586346\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /home/crueang/Chaks/AIOT_project/democp/prune/global_prune/v2 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, optimizer, criterion, train_loader, test_loader, \u001b[38;5;241m50\u001b[39m, checkpoint_path, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[24], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, opt, loss_fn, train_loader, test_loader, epochs, checkpoint_path, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_forward_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     74\u001b[0m         module\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m---> 75\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39meval(), checkpoint_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(opt\u001b[38;5;241m.\u001b[39mstate_dict(), checkpoint_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(training_logs, checkpoint_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_logs.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:525\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:496\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /home/crueang/Chaks/AIOT_project/democp/prune/global_prune/v2 does not exist."
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_loader, test_loader, 50, checkpoint_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 96.787149 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "dslr_dataset = datasets.ImageFolder(root=f'{DATASET_DIR}/dslr', transform=transformer)\n",
    "Dl_dslr = DataLoader(dslr_dataset, batch_size, shuffle=False)\n",
    "\n",
    "model = torch.load('/home/crueang/Chaks/AIOT_project/democp/prune/global_prune/v1/best_model.pth', weights_only=False, map_location=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in Dl_dslr:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        correct += (outputs.argmax(1) == labels).float().sum().item()\n",
    "        total += labels.shape[0]\n",
    "        \n",
    "\n",
    "print('Accuracy : %f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_logs(checkpoint_path):\n",
    "    training_logs = torch.load(checkpoint_path + 'training_logs.pth', weights_only=True)\n",
    "    epoch_number = len(training_logs['train_loss'])\n",
    "    \n",
    "    for i in range(min(epoch_number, 50)):\n",
    "        print(f\"Epochs {i+1}\".ljust(10), end='')\n",
    "        for k, v in training_logs.items():\n",
    "            print(f\"{k}: {v[i]:.5f}\", end=\" \")\n",
    "        print()\n",
    "    \n",
    "    print('best test acc', max(training_logs['validate_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1  train_loss: 0.07780 validate_loss: 0.04253 train_acc: 0.65815 validate_acc: 0.81582 \n",
      "Epochs 2  train_loss: 0.03737 validate_loss: 0.01691 train_acc: 0.83844 validate_acc: 0.91995 \n",
      "Epochs 3  train_loss: 0.02680 validate_loss: 0.03695 train_acc: 0.87591 validate_acc: 0.84745 \n",
      "Epochs 4  train_loss: 0.02357 validate_loss: 0.01864 train_acc: 0.89367 validate_acc: 0.91192 \n",
      "Epochs 5  train_loss: 0.02091 validate_loss: 0.03554 train_acc: 0.90438 validate_acc: 0.85401 \n",
      "Epochs 6  train_loss: 0.01458 validate_loss: 0.01169 train_acc: 0.92968 validate_acc: 0.95182 \n",
      "Epochs 7  train_loss: 0.01558 validate_loss: 0.00652 train_acc: 0.92336 validate_acc: 0.96813 \n",
      "Epochs 8  train_loss: 0.01149 validate_loss: 0.01724 train_acc: 0.94599 validate_acc: 0.92798 \n",
      "Epochs 9  train_loss: 0.01572 validate_loss: 0.01213 train_acc: 0.93236 validate_acc: 0.94842 \n",
      "Epochs 10 train_loss: 0.00967 validate_loss: 0.00932 train_acc: 0.94964 validate_acc: 0.95864 \n",
      "Epochs 11 train_loss: 0.01036 validate_loss: 0.00964 train_acc: 0.95109 validate_acc: 0.95742 \n",
      "Epochs 12 train_loss: 0.00916 validate_loss: 0.00519 train_acc: 0.95888 validate_acc: 0.97664 \n",
      "Epochs 13 train_loss: 0.00986 validate_loss: 0.00700 train_acc: 0.95328 validate_acc: 0.96886 \n",
      "Epochs 14 train_loss: 0.01076 validate_loss: 0.01668 train_acc: 0.95036 validate_acc: 0.93577 \n",
      "Epochs 15 train_loss: 0.01031 validate_loss: 0.01330 train_acc: 0.95572 validate_acc: 0.95085 \n",
      "Epochs 16 train_loss: 0.00845 validate_loss: 0.00370 train_acc: 0.96521 validate_acc: 0.98005 \n",
      "Epochs 17 train_loss: 0.00692 validate_loss: 0.00203 train_acc: 0.96983 validate_acc: 0.99075 \n",
      "Epochs 18 train_loss: 0.00640 validate_loss: 0.01193 train_acc: 0.97251 validate_acc: 0.94988 \n",
      "Epochs 19 train_loss: 0.01157 validate_loss: 0.01705 train_acc: 0.94647 validate_acc: 0.93723 \n",
      "Epochs 20 train_loss: 0.01005 validate_loss: 0.00576 train_acc: 0.95693 validate_acc: 0.97835 \n",
      "best test acc 0.9907542579075426\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = '/home/crueang/Chaks/AIOT_project/cp/mbv3_large_ref/'\n",
    "\n",
    "load_training_logs(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
