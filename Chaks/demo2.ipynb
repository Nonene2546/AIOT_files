{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/crueang/Chaks/AIOT_project/data/Office-31'\n",
    "HOME = '/home/crueang/Chaks/AIOT_project'\n",
    "CP = '/home/crueang/Chaks/AIOT_project/democp'\n",
    "\n",
    "NUM_CLASSES = 31\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back_pack', 'bike', 'bike_helmet', 'bookcase', 'bottle', 'calculator', 'desk_chair', 'desk_lamp', 'desktop_computer', 'file_cabinet', 'headphones', 'keyboard', 'laptop_computer', 'letter_tray', 'mobile_phone', 'monitor', 'mouse', 'mug', 'paper_notebook', 'pen', 'phone', 'printer', 'projector', 'punchers', 'ring_binder', 'ruler', 'scissors', 'speaker', 'stapler', 'tape_dispenser', 'trash_can']\n",
      "2817 498 795\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.models import mobilenet_v3_large, mobilenet_v3_small\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    torchvision.models.MobileNet_V3_Small_Weights.IMAGENET1K_V1.transforms(),\n",
    "])\n",
    "\n",
    "amazon_dataset = ImageFolder(root=f'{DATASET_DIR}/amazon', transform=transform)\n",
    "dslr_dataset = ImageFolder(root=f'{DATASET_DIR}/dslr', transform=transform)\n",
    "webcam_dataset = ImageFolder(root=f'{DATASET_DIR}/webcam', transform=transform)\n",
    "\n",
    "classes = amazon_dataset.classes\n",
    "\n",
    "print(classes)\n",
    "\n",
    "print(len(amazon_dataset), len(dslr_dataset), len(webcam_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kd\n",
    "temperature, alpha, l2_weight = 3, 0.9, 0\n",
    "\n",
    "# mmd\n",
    "mmd_alpha = 1\n",
    "\n",
    "# coral\n",
    "lambda_coral = 1\n",
    "\n",
    "# pixel mix\n",
    "use_pixel_mix = False\n",
    "\n",
    "# src\n",
    "src_weight = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title FLOPS computation\n",
    "# Code from https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def print_model_param_nums(model=None):\n",
    "    if model == None:\n",
    "        model = torchvision.models.alexnet()\n",
    "    total = sum([param.nelement() if param.requires_grad else 0 for param in model.parameters()])\n",
    "    print('  + Number of params: %.4fM' % (total / 1e6))\n",
    "\n",
    "def count_model_param_flops(model=None, input_res=224, multiply_adds=True, device='cpu'):\n",
    "\n",
    "    prods = {}\n",
    "    def save_hook(name):\n",
    "        def hook_per(self, input, output):\n",
    "            prods[name] = np.prod(input[0].shape)\n",
    "        return hook_per\n",
    "\n",
    "    list_1=[]\n",
    "    def simple_hook(self, input, output):\n",
    "        list_1.append(np.prod(input[0].shape))\n",
    "    list_2={}\n",
    "    def simple_hook2(self, input, output):\n",
    "        list_2['names'] = np.prod(input[0].shape)\n",
    "\n",
    "\n",
    "    list_conv=[]\n",
    "    def conv_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    "\n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        # flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n",
    "\n",
    "        num_weight_params = (self.weight.data != 0).float().sum()\n",
    "        flops = (num_weight_params * (2 if multiply_adds else 1) + bias_ops * output_channels) * output_height * output_width * batch_size\n",
    "\n",
    "        list_conv.append(flops)\n",
    "\n",
    "    list_linear=[]\n",
    "    def linear_hook(self, input, output):\n",
    "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
    "\n",
    "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
    "        bias_ops = self.bias.nelement()\n",
    "\n",
    "        flops = batch_size * (weight_ops + bias_ops)\n",
    "        list_linear.append(flops)\n",
    "\n",
    "    list_bn=[]\n",
    "    def bn_hook(self, input, output):\n",
    "        list_bn.append(input[0].nelement() * 2)\n",
    "\n",
    "    list_relu=[]\n",
    "    def relu_hook(self, input, output):\n",
    "        list_relu.append(input[0].nelement())\n",
    "\n",
    "    list_pooling=[]\n",
    "    def pooling_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        kernel_ops = self.kernel_size * self.kernel_size\n",
    "        bias_ops = 0\n",
    "        params = 0\n",
    "        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n",
    "\n",
    "        list_pooling.append(flops)\n",
    "\n",
    "    list_upsample=[]\n",
    "\n",
    "    # For bilinear upsample\n",
    "    def upsample_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "\n",
    "        flops = output_height * output_width * output_channels * batch_size * 12\n",
    "        list_upsample.append(flops)\n",
    "\n",
    "    def foo(net):\n",
    "        childrens = list(net.children())\n",
    "        if not childrens:\n",
    "            if isinstance(net, torch.nn.Conv2d):\n",
    "                net.register_forward_hook(conv_hook)\n",
    "            if isinstance(net, torch.nn.Linear):\n",
    "                net.register_forward_hook(linear_hook)\n",
    "            if isinstance(net, torch.nn.BatchNorm2d):\n",
    "                net.register_forward_hook(bn_hook)\n",
    "            if isinstance(net, torch.nn.ReLU):\n",
    "                net.register_forward_hook(relu_hook)\n",
    "            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
    "                net.register_forward_hook(pooling_hook)\n",
    "            if isinstance(net, torch.nn.Upsample):\n",
    "                net.register_forward_hook(upsample_hook)\n",
    "            return\n",
    "        for c in childrens:\n",
    "            foo(c)\n",
    "\n",
    "    if model == None:\n",
    "        model = torchvision.models.alexnet()\n",
    "    foo(model)\n",
    "    input = Variable(torch.rand(3,input_res,input_res).unsqueeze(0), requires_grad = True).to(device)\n",
    "    out = model(input)\n",
    "\n",
    "\n",
    "    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n",
    "\n",
    "    print('Number of FLOPs: %.6f GFLOPs (%.2f MFLOPs)' % (total_flops / 1e9, total_flops / 1e6))\n",
    "\n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path = f'{CP}/pretrained_mmd25/best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 112, 112]             216\n",
      "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
      "         Hardswish-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4            [-1, 8, 56, 56]              72\n",
      "       BatchNorm2d-5            [-1, 8, 56, 56]              16\n",
      "              ReLU-6            [-1, 8, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7              [-1, 8, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]              72\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10              [-1, 8, 1, 1]              72\n",
      "      Hardsigmoid-11              [-1, 8, 1, 1]               0\n",
      "SqueezeExcitation-12            [-1, 8, 56, 56]               0\n",
      "           Conv2d-13            [-1, 8, 56, 56]              64\n",
      "      BatchNorm2d-14            [-1, 8, 56, 56]              16\n",
      " InvertedResidual-15            [-1, 8, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]             192\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      "             ReLU-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-20           [-1, 24, 28, 28]              48\n",
      "             ReLU-21           [-1, 24, 28, 28]               0\n",
      "           Conv2d-22            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-23            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-24            [-1, 8, 28, 28]               0\n",
      "           Conv2d-25           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-26           [-1, 24, 28, 28]              48\n",
      "             ReLU-27           [-1, 24, 28, 28]               0\n",
      "           Conv2d-28           [-1, 24, 28, 28]             216\n",
      "      BatchNorm2d-29           [-1, 24, 28, 28]              48\n",
      "             ReLU-30           [-1, 24, 28, 28]               0\n",
      "           Conv2d-31            [-1, 8, 28, 28]             192\n",
      "      BatchNorm2d-32            [-1, 8, 28, 28]              16\n",
      " InvertedResidual-33            [-1, 8, 28, 28]               0\n",
      "           Conv2d-34           [-1, 24, 28, 28]             192\n",
      "      BatchNorm2d-35           [-1, 24, 28, 28]              48\n",
      "        Hardswish-36           [-1, 24, 28, 28]               0\n",
      "           Conv2d-37           [-1, 24, 14, 14]             600\n",
      "      BatchNorm2d-38           [-1, 24, 14, 14]              48\n",
      "        Hardswish-39           [-1, 24, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 24, 1, 1]               0\n",
      "           Conv2d-41              [-1, 8, 1, 1]             200\n",
      "             ReLU-42              [-1, 8, 1, 1]               0\n",
      "           Conv2d-43             [-1, 24, 1, 1]             216\n",
      "      Hardsigmoid-44             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 24, 14, 14]               0\n",
      "           Conv2d-46           [-1, 16, 14, 14]             384\n",
      "      BatchNorm2d-47           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-48           [-1, 16, 14, 14]               0\n",
      "           Conv2d-49           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-50           [-1, 64, 14, 14]             128\n",
      "        Hardswish-51           [-1, 64, 14, 14]               0\n",
      "           Conv2d-52           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-53           [-1, 64, 14, 14]             128\n",
      "        Hardswish-54           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55             [-1, 64, 1, 1]               0\n",
      "           Conv2d-56             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-57             [-1, 16, 1, 1]               0\n",
      "           Conv2d-58             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-59             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-60           [-1, 64, 14, 14]               0\n",
      "           Conv2d-61           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-62           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-63           [-1, 16, 14, 14]               0\n",
      "           Conv2d-64           [-1, 64, 14, 14]           1,024\n",
      "      BatchNorm2d-65           [-1, 64, 14, 14]             128\n",
      "        Hardswish-66           [-1, 64, 14, 14]               0\n",
      "           Conv2d-67           [-1, 64, 14, 14]           1,600\n",
      "      BatchNorm2d-68           [-1, 64, 14, 14]             128\n",
      "        Hardswish-69           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n",
      "           Conv2d-71             [-1, 16, 1, 1]           1,040\n",
      "             ReLU-72             [-1, 16, 1, 1]               0\n",
      "           Conv2d-73             [-1, 64, 1, 1]           1,088\n",
      "      Hardsigmoid-74             [-1, 64, 1, 1]               0\n",
      "SqueezeExcitation-75           [-1, 64, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           1,024\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-78           [-1, 16, 14, 14]               0\n",
      "           Conv2d-79           [-1, 32, 14, 14]             512\n",
      "      BatchNorm2d-80           [-1, 32, 14, 14]              64\n",
      "        Hardswish-81           [-1, 32, 14, 14]               0\n",
      "           Conv2d-82           [-1, 32, 14, 14]             800\n",
      "      BatchNorm2d-83           [-1, 32, 14, 14]              64\n",
      "        Hardswish-84           [-1, 32, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85             [-1, 32, 1, 1]               0\n",
      "           Conv2d-86              [-1, 8, 1, 1]             264\n",
      "             ReLU-87              [-1, 8, 1, 1]               0\n",
      "           Conv2d-88             [-1, 32, 1, 1]             288\n",
      "      Hardsigmoid-89             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-90           [-1, 32, 14, 14]               0\n",
      "           Conv2d-91           [-1, 16, 14, 14]             512\n",
      "      BatchNorm2d-92           [-1, 16, 14, 14]              32\n",
      " InvertedResidual-93           [-1, 16, 14, 14]               0\n",
      "           Conv2d-94           [-1, 40, 14, 14]             640\n",
      "      BatchNorm2d-95           [-1, 40, 14, 14]              80\n",
      "        Hardswish-96           [-1, 40, 14, 14]               0\n",
      "           Conv2d-97           [-1, 40, 14, 14]           1,000\n",
      "      BatchNorm2d-98           [-1, 40, 14, 14]              80\n",
      "        Hardswish-99           [-1, 40, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100             [-1, 40, 1, 1]               0\n",
      "          Conv2d-101             [-1, 16, 1, 1]             656\n",
      "            ReLU-102             [-1, 16, 1, 1]               0\n",
      "          Conv2d-103             [-1, 40, 1, 1]             680\n",
      "     Hardsigmoid-104             [-1, 40, 1, 1]               0\n",
      "SqueezeExcitation-105           [-1, 40, 14, 14]               0\n",
      "          Conv2d-106           [-1, 16, 14, 14]             640\n",
      "     BatchNorm2d-107           [-1, 16, 14, 14]              32\n",
      "InvertedResidual-108           [-1, 16, 14, 14]               0\n",
      "          Conv2d-109           [-1, 72, 14, 14]           1,152\n",
      "     BatchNorm2d-110           [-1, 72, 14, 14]             144\n",
      "       Hardswish-111           [-1, 72, 14, 14]               0\n",
      "          Conv2d-112             [-1, 72, 7, 7]           1,800\n",
      "     BatchNorm2d-113             [-1, 72, 7, 7]             144\n",
      "       Hardswish-114             [-1, 72, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 72, 1, 1]               0\n",
      "          Conv2d-116             [-1, 24, 1, 1]           1,752\n",
      "            ReLU-117             [-1, 24, 1, 1]               0\n",
      "          Conv2d-118             [-1, 72, 1, 1]           1,800\n",
      "     Hardsigmoid-119             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-120             [-1, 72, 7, 7]               0\n",
      "          Conv2d-121             [-1, 24, 7, 7]           1,728\n",
      "     BatchNorm2d-122             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-123             [-1, 24, 7, 7]               0\n",
      "          Conv2d-124            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-125            [-1, 144, 7, 7]             288\n",
      "       Hardswish-126            [-1, 144, 7, 7]               0\n",
      "          Conv2d-127            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-128            [-1, 144, 7, 7]             288\n",
      "       Hardswish-129            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 144, 1, 1]               0\n",
      "          Conv2d-131             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-132             [-1, 40, 1, 1]               0\n",
      "          Conv2d-133            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-134            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 144, 7, 7]               0\n",
      "          Conv2d-136             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-137             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-138             [-1, 24, 7, 7]               0\n",
      "          Conv2d-139            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-140            [-1, 144, 7, 7]             288\n",
      "       Hardswish-141            [-1, 144, 7, 7]               0\n",
      "          Conv2d-142            [-1, 144, 7, 7]           3,600\n",
      "     BatchNorm2d-143            [-1, 144, 7, 7]             288\n",
      "       Hardswish-144            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 144, 1, 1]               0\n",
      "          Conv2d-146             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-147             [-1, 40, 1, 1]               0\n",
      "          Conv2d-148            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-149            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 144, 7, 7]               0\n",
      "          Conv2d-151             [-1, 24, 7, 7]           3,456\n",
      "     BatchNorm2d-152             [-1, 24, 7, 7]              48\n",
      "InvertedResidual-153             [-1, 24, 7, 7]               0\n",
      "          Conv2d-154            [-1, 144, 7, 7]           3,456\n",
      "     BatchNorm2d-155            [-1, 144, 7, 7]             288\n",
      "       Hardswish-156            [-1, 144, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 144, 1, 1]               0\n",
      "          Linear-158                  [-1, 256]          37,120\n",
      "       Hardswish-159                  [-1, 256]               0\n",
      "         Dropout-160                  [-1, 256]               0\n",
      "          Linear-161                   [-1, 31]           7,967\n",
      "================================================================\n",
      "Total params: 125,239\n",
      "Trainable params: 125,239\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 11.42\n",
      "Params size (MB): 0.48\n",
      "Estimated Total Size (MB): 12.47\n",
      "----------------------------------------------------------------\n",
      "Number of FLOPs: 0.018660 GFLOPs (18.66 MFLOPs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(18659808.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model = torch.load('/home/crueang/Chaks/AIOT_project/best_resnet_amazon.pth', weights_only=False, map_location=device)\n",
    "teacher_model.eval()\n",
    "model = torchvision.models.mobilenet_v3_small(weights=None, width_mult=0.25, num_classes=31).to(device)\n",
    "if resume_path:\n",
    "    model.load_state_dict(torch.load(resume_path, weights_only=True, map_location=device))\n",
    "summary(model, (3, 224, 224))\n",
    "count_model_param_flops(model=model.cpu().eval(), input_res=224, multiply_adds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_before(model, n):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'features' in name and int(name.split('.')[1]) < n:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "        print(f'Layer: {name}, requires_grad: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=1.0, alpha=0.9, l2_weight=0.01):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.l2_weight = l2_weight\n",
    "        self.criterion_soft = nn.KLDivLoss(reduction='batchmean')  # Soft targets\n",
    "        self.criterion_hard = nn.CrossEntropyLoss()  # Hard targets\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels, student_params, teacher_params):\n",
    "        # Soft loss (knowledge distillation)\n",
    "        soft_student_probs = torch.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        soft_teacher_probs = torch.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        soft_loss = self.criterion_soft(soft_student_probs, soft_teacher_probs)\n",
    "\n",
    "        # Hard loss (ground truth labels)\n",
    "        hard_loss = self.criterion_hard(student_logits, labels)\n",
    "\n",
    "        # # L2 Loss to match student parameters to teacher's parameters\n",
    "        l2_loss = 0.0\n",
    "        for student_param, teacher_param in zip(student_params, teacher_params):\n",
    "            try:\n",
    "                l2_loss += torch.norm(student_param - teacher_param, p=2)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = (self.alpha * soft_loss + (1 - self.alpha) * hard_loss) + self.l2_weight * l2_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationAlignmentLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CorrelationAlignmentLoss, self).__init__()\n",
    "\n",
    "    def forward(self, f_s: torch.Tensor, f_t: torch.Tensor) -> torch.Tensor:\n",
    "        mean_s = f_s.mean(0, keepdim=True)\n",
    "        mean_t = f_t.mean(0, keepdim=True)\n",
    "        cent_s = f_s - mean_s\n",
    "        cent_t = f_t - mean_t\n",
    "        cov_s = torch.mm(cent_s.t(), cent_s) / (len(f_s) - 1)\n",
    "        cov_t = torch.mm(cent_t.t(), cent_t) / (len(f_t) - 1)\n",
    "\n",
    "        mean_diff = (mean_s - mean_t).pow(2).mean()\n",
    "        cov_diff = (cov_s - cov_t).pow(2).mean()\n",
    "\n",
    "        return mean_diff + cov_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmd loss    \n",
    "def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "    total = torch.cat([source, target], dim=0)\n",
    "    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "    L2_distance = ((total0-total1)**2).sum(2)\n",
    "    if fix_sigma:\n",
    "        bandwidth = fix_sigma\n",
    "    else:\n",
    "        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "    bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "    return sum(kernel_val)#/len(kernel_val)\n",
    "\n",
    "def mmdloss(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    batch_size = int(source.size()[0])\n",
    "    kernels = guassian_kernel(source, target,\n",
    "                              kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "    XX = kernels[:batch_size, :batch_size]\n",
    "    YY = kernels[batch_size:, batch_size:]\n",
    "    XY = kernels[:batch_size, batch_size:]\n",
    "    YX = kernels[batch_size:, :batch_size]\n",
    "    loss = torch.mean(XX + YY - XY -YX)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel mix\n",
    "import random\n",
    "def pixel_mix(source, target, alpha=0.9):\n",
    "    # Get the dimensions of the images\n",
    "    batch_size, _, height, width = target.size()\n",
    "\n",
    "    # Create an empty tensor for mixed images\n",
    "    mixed_images = torch.empty_like(target)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                # Randomly choose between source and target pixels\n",
    "                if random.random() < alpha:\n",
    "                    mixed_images[i, :, h, w] = source[i, :, h, w]\n",
    "                else:\n",
    "                    mixed_images[i, :, h, w] = target[i, :, h, w]\n",
    "\n",
    "    return mixed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.0.weight, requires_grad: True\n",
      "Layer: features.0.1.weight, requires_grad: True\n",
      "Layer: features.0.1.bias, requires_grad: True\n",
      "Layer: features.1.block.0.0.weight, requires_grad: True\n",
      "Layer: features.1.block.0.1.weight, requires_grad: True\n",
      "Layer: features.1.block.0.1.bias, requires_grad: True\n",
      "Layer: features.1.block.1.fc1.weight, requires_grad: True\n",
      "Layer: features.1.block.1.fc1.bias, requires_grad: True\n",
      "Layer: features.1.block.1.fc2.weight, requires_grad: True\n",
      "Layer: features.1.block.1.fc2.bias, requires_grad: True\n",
      "Layer: features.1.block.2.0.weight, requires_grad: True\n",
      "Layer: features.1.block.2.1.weight, requires_grad: True\n",
      "Layer: features.1.block.2.1.bias, requires_grad: True\n",
      "Layer: features.2.block.0.0.weight, requires_grad: True\n",
      "Layer: features.2.block.0.1.weight, requires_grad: True\n",
      "Layer: features.2.block.0.1.bias, requires_grad: True\n",
      "Layer: features.2.block.1.0.weight, requires_grad: True\n",
      "Layer: features.2.block.1.1.weight, requires_grad: True\n",
      "Layer: features.2.block.1.1.bias, requires_grad: True\n",
      "Layer: features.2.block.2.0.weight, requires_grad: True\n",
      "Layer: features.2.block.2.1.weight, requires_grad: True\n",
      "Layer: features.2.block.2.1.bias, requires_grad: True\n",
      "Layer: features.3.block.0.0.weight, requires_grad: True\n",
      "Layer: features.3.block.0.1.weight, requires_grad: True\n",
      "Layer: features.3.block.0.1.bias, requires_grad: True\n",
      "Layer: features.3.block.1.0.weight, requires_grad: True\n",
      "Layer: features.3.block.1.1.weight, requires_grad: True\n",
      "Layer: features.3.block.1.1.bias, requires_grad: True\n",
      "Layer: features.3.block.2.0.weight, requires_grad: True\n",
      "Layer: features.3.block.2.1.weight, requires_grad: True\n",
      "Layer: features.3.block.2.1.bias, requires_grad: True\n",
      "Layer: features.4.block.0.0.weight, requires_grad: True\n",
      "Layer: features.4.block.0.1.weight, requires_grad: True\n",
      "Layer: features.4.block.0.1.bias, requires_grad: True\n",
      "Layer: features.4.block.1.0.weight, requires_grad: True\n",
      "Layer: features.4.block.1.1.weight, requires_grad: True\n",
      "Layer: features.4.block.1.1.bias, requires_grad: True\n",
      "Layer: features.4.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.4.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.4.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.4.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.4.block.3.0.weight, requires_grad: True\n",
      "Layer: features.4.block.3.1.weight, requires_grad: True\n",
      "Layer: features.4.block.3.1.bias, requires_grad: True\n",
      "Layer: features.5.block.0.0.weight, requires_grad: True\n",
      "Layer: features.5.block.0.1.weight, requires_grad: True\n",
      "Layer: features.5.block.0.1.bias, requires_grad: True\n",
      "Layer: features.5.block.1.0.weight, requires_grad: True\n",
      "Layer: features.5.block.1.1.weight, requires_grad: True\n",
      "Layer: features.5.block.1.1.bias, requires_grad: True\n",
      "Layer: features.5.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.5.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.5.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.5.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.5.block.3.0.weight, requires_grad: True\n",
      "Layer: features.5.block.3.1.weight, requires_grad: True\n",
      "Layer: features.5.block.3.1.bias, requires_grad: True\n",
      "Layer: features.6.block.0.0.weight, requires_grad: True\n",
      "Layer: features.6.block.0.1.weight, requires_grad: True\n",
      "Layer: features.6.block.0.1.bias, requires_grad: True\n",
      "Layer: features.6.block.1.0.weight, requires_grad: True\n",
      "Layer: features.6.block.1.1.weight, requires_grad: True\n",
      "Layer: features.6.block.1.1.bias, requires_grad: True\n",
      "Layer: features.6.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.6.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.6.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.6.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.6.block.3.0.weight, requires_grad: True\n",
      "Layer: features.6.block.3.1.weight, requires_grad: True\n",
      "Layer: features.6.block.3.1.bias, requires_grad: True\n",
      "Layer: features.7.block.0.0.weight, requires_grad: True\n",
      "Layer: features.7.block.0.1.weight, requires_grad: True\n",
      "Layer: features.7.block.0.1.bias, requires_grad: True\n",
      "Layer: features.7.block.1.0.weight, requires_grad: True\n",
      "Layer: features.7.block.1.1.weight, requires_grad: True\n",
      "Layer: features.7.block.1.1.bias, requires_grad: True\n",
      "Layer: features.7.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.7.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.7.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.7.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.7.block.3.0.weight, requires_grad: True\n",
      "Layer: features.7.block.3.1.weight, requires_grad: True\n",
      "Layer: features.7.block.3.1.bias, requires_grad: True\n",
      "Layer: features.8.block.0.0.weight, requires_grad: True\n",
      "Layer: features.8.block.0.1.weight, requires_grad: True\n",
      "Layer: features.8.block.0.1.bias, requires_grad: True\n",
      "Layer: features.8.block.1.0.weight, requires_grad: True\n",
      "Layer: features.8.block.1.1.weight, requires_grad: True\n",
      "Layer: features.8.block.1.1.bias, requires_grad: True\n",
      "Layer: features.8.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.8.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.8.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.8.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.8.block.3.0.weight, requires_grad: True\n",
      "Layer: features.8.block.3.1.weight, requires_grad: True\n",
      "Layer: features.8.block.3.1.bias, requires_grad: True\n",
      "Layer: features.9.block.0.0.weight, requires_grad: True\n",
      "Layer: features.9.block.0.1.weight, requires_grad: True\n",
      "Layer: features.9.block.0.1.bias, requires_grad: True\n",
      "Layer: features.9.block.1.0.weight, requires_grad: True\n",
      "Layer: features.9.block.1.1.weight, requires_grad: True\n",
      "Layer: features.9.block.1.1.bias, requires_grad: True\n",
      "Layer: features.9.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.9.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.9.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.9.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.9.block.3.0.weight, requires_grad: True\n",
      "Layer: features.9.block.3.1.weight, requires_grad: True\n",
      "Layer: features.9.block.3.1.bias, requires_grad: True\n",
      "Layer: features.10.block.0.0.weight, requires_grad: True\n",
      "Layer: features.10.block.0.1.weight, requires_grad: True\n",
      "Layer: features.10.block.0.1.bias, requires_grad: True\n",
      "Layer: features.10.block.1.0.weight, requires_grad: True\n",
      "Layer: features.10.block.1.1.weight, requires_grad: True\n",
      "Layer: features.10.block.1.1.bias, requires_grad: True\n",
      "Layer: features.10.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.10.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.10.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.10.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.10.block.3.0.weight, requires_grad: True\n",
      "Layer: features.10.block.3.1.weight, requires_grad: True\n",
      "Layer: features.10.block.3.1.bias, requires_grad: True\n",
      "Layer: features.11.block.0.0.weight, requires_grad: True\n",
      "Layer: features.11.block.0.1.weight, requires_grad: True\n",
      "Layer: features.11.block.0.1.bias, requires_grad: True\n",
      "Layer: features.11.block.1.0.weight, requires_grad: True\n",
      "Layer: features.11.block.1.1.weight, requires_grad: True\n",
      "Layer: features.11.block.1.1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.weight, requires_grad: True\n",
      "Layer: features.11.block.2.fc1.bias, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.weight, requires_grad: True\n",
      "Layer: features.11.block.2.fc2.bias, requires_grad: True\n",
      "Layer: features.11.block.3.0.weight, requires_grad: True\n",
      "Layer: features.11.block.3.1.weight, requires_grad: True\n",
      "Layer: features.11.block.3.1.bias, requires_grad: True\n",
      "Layer: features.12.0.weight, requires_grad: True\n",
      "Layer: features.12.1.weight, requires_grad: True\n",
      "Layer: features.12.1.bias, requires_grad: True\n",
      "Layer: classifier.0.weight, requires_grad: True\n",
      "Layer: classifier.0.bias, requires_grad: True\n",
      "Layer: classifier.3.weight, requires_grad: True\n",
      "Layer: classifier.3.bias, requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "freeze_before(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(amazon_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dslr_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "distillation_loss = DistillationLoss(temperature, alpha, l2_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint_path = f'{CP}/pretrained_mmd26/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_student_2\n",
    "from tqdm import tqdm\n",
    "def train_da(teacher_model, student_model, opt, loss_fn, distillation_loss, Dl_source,Dl_target ,test_loader, epochs=10, checkpoint_path=None, device='cpu'):\n",
    "    training_logs = {\"train_loss\": [], \"validate_loss\": [], \"train_acc\": [], \"validate_acc\": []}\n",
    "    epoch_number = 0\n",
    "    best_test_loss = float('inf')\n",
    "    student_model = student_model.to(device)\n",
    "    # alpha_loss = 0.1\n",
    "    if checkpoint_path:\n",
    "      if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "        \n",
    "      if os.path.exists(checkpoint_path + 'model.pth'):\n",
    "        student_model.load_state_dict(torch.load(checkpoint_path + 'model.pth', weights_only=True, map_location=device))\n",
    "\n",
    "      if os.path.exists(checkpoint_path + 'opt.pth'):\n",
    "        opt.load_state_dict(torch.load(checkpoint_path + 'opt.pth', weights_only=True, map_location=device))\n",
    "\n",
    "      if os.path.exists(checkpoint_path + 'training_logs.pth'):\n",
    "        training_logs = torch.load(checkpoint_path + 'training_logs.pth', weights_only=True)\n",
    "        epoch_number = len(training_logs['train_loss'])\n",
    "        best_test_loss = min(best_test_loss, min(training_logs['validate_loss']))\n",
    "        print(max(training_logs['validate_acc']))\n",
    "\n",
    "    for i in range(epoch_number):\n",
    "        print(f\"Epochs {i+1}\".ljust(10), end='')\n",
    "        for k, v in training_logs.items():\n",
    "            print(f\"{k}: {v[i]:.5f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "    print(\"ðŸ¤–Training on\", device)\n",
    "    max_batch = min(len(Dl_source),len(Dl_target))\n",
    "    for epoch in range(epoch_number, epochs):\n",
    "\n",
    "        train_loss, train_correct, mmd = 0, 0, 0\n",
    "        student_model.train()\n",
    "        print(f'epochs {epoch+1:04d} / {epochs:04d}', end='\\n============\\n')\n",
    "        Dl_source_iter = iter(Dl_source)\n",
    "        Dl_target_iter = iter(Dl_target)\n",
    "        p = (epoch + 1) / epochs\n",
    "        \n",
    "        for batch_idx in range(max_batch):\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            X_s, y_s = next(Dl_source_iter)\n",
    "\n",
    "            X_t, y_t = next(Dl_target_iter)\n",
    "            \n",
    "            if use_pixel_mix:\n",
    "              X_s = pixel_mix(X_s, X_t)\n",
    "\n",
    "            if X_s.shape[0] != X_t.shape[0]:\n",
    "              min_bs = min(X_s.shape[0], X_t.shape[0])\n",
    "              X_s = X_s[:min_bs]\n",
    "              y_s = y_s[:min_bs]\n",
    "              X_t = X_t[:min_bs]\n",
    "              y_t = y_t[:min_bs]\n",
    "            \n",
    "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
    "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
    "\n",
    "            class_pred_s = student_model(X_s)\n",
    "            class_pred_t = student_model(X_t)\n",
    "\n",
    "            loss_src = loss_fn(class_pred_s, y_s) \n",
    "            mmd_loss = mmdloss(class_pred_s, class_pred_t)*mmd_alpha\n",
    "            \n",
    "             # Knowledge distillation loss\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher_model(X_s)\n",
    "                \n",
    "            student_params = list(model.parameters())\n",
    "            teacher_params = list(teacher_model.parameters())\n",
    "                \n",
    "            loss_distillation = distillation_loss(class_pred_s, teacher_output, y_s, student_params, teacher_params)\n",
    "            \n",
    "            loss_coral = CorrelationAlignmentLoss()(class_pred_s, class_pred_t)*lambda_coral\n",
    "\n",
    "            loss = loss_src*src_weight + mmd_loss + loss_distillation + loss_coral\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            mmd += mmd_loss.item()\n",
    "            with torch.no_grad():\n",
    "                class_prediction_S = student_model(X_s)\n",
    "                train_correct_s = (class_prediction_S.argmax(1) == y_s).float().sum().item()\n",
    "                class_prediction_T = student_model(X_t)\n",
    "                train_correct_t = (class_prediction_T.argmax(1) == y_t).float().sum().item()\n",
    "                \n",
    "            train_correct += train_correct_s\n",
    "\n",
    "        avg_train_loss = train_loss / (max_batch*BATCH_SIZE)\n",
    "        avg_train_acc = train_correct / (max_batch*BATCH_SIZE)\n",
    "        print(f'\\n\\tTrain loss: {avg_train_loss}')\n",
    "        print(f'\\tTrain acc: {avg_train_acc}')\n",
    "        training_logs[\"train_loss\"].append(avg_train_loss)\n",
    "        training_logs[\"train_acc\"].append(avg_train_acc)\n",
    "\n",
    "        test_loss, test_correct = 0, 0\n",
    "        student_model.eval()\n",
    "        test_bar = tqdm(test_loader , desc='ðŸ“„Testing' , unit='batch')\n",
    "        with torch.no_grad():\n",
    "          for images, label in test_bar:\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            outputs = student_model(images)\n",
    "            loss = loss_fn(outputs, label)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += (outputs.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "        avg_test_acc = test_correct / len(test_loader.dataset)\n",
    "        print(f'\\tTest loss: {avg_test_loss}')\n",
    "        print(f'\\tTest acc: {avg_test_acc}')\n",
    "        training_logs[\"validate_loss\"].append(avg_test_loss)\n",
    "        training_logs[\"validate_acc\"].append(avg_test_acc)\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save(student_model.state_dict(), checkpoint_path + \"model.pth\")\n",
    "            torch.save(opt.state_dict(), checkpoint_path + \"opt.pth\")\n",
    "            torch.save(training_logs, checkpoint_path + 'training_logs.pth')\n",
    "            if best_test_loss > avg_test_loss:\n",
    "               torch.save(student_model.state_dict(), checkpoint_path + \"best_model.pth\")\n",
    "               best_test_loss = avg_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618473895582329\n",
      "Epochs 1  train_loss: 1.55582 validate_loss: 0.09290 train_acc: 0.68164 validate_acc: 0.65663 \n",
      "Epochs 2  train_loss: 1.47441 validate_loss: 0.08451 train_acc: 0.63672 validate_acc: 0.70080 \n",
      "Epochs 3  train_loss: 1.39879 validate_loss: 0.07612 train_acc: 0.67188 validate_acc: 0.71084 \n",
      "Epochs 4  train_loss: 1.32647 validate_loss: 0.07450 train_acc: 0.69531 validate_acc: 0.74498 \n",
      "Epochs 5  train_loss: 1.25006 validate_loss: 0.06742 train_acc: 0.73633 validate_acc: 0.73896 \n",
      "Epochs 6  train_loss: 1.20225 validate_loss: 0.07854 train_acc: 0.69531 validate_acc: 0.69679 \n",
      "Epochs 7  train_loss: 1.16494 validate_loss: 0.06406 train_acc: 0.69531 validate_acc: 0.75703 \n",
      "Epochs 8  train_loss: 1.10018 validate_loss: 0.05328 train_acc: 0.73242 validate_acc: 0.78112 \n",
      "Epochs 9  train_loss: 1.05939 validate_loss: 0.04555 train_acc: 0.72070 validate_acc: 0.77711 \n",
      "Epochs 10 train_loss: 1.02230 validate_loss: 0.04772 train_acc: 0.72852 validate_acc: 0.79317 \n",
      "Epochs 11 train_loss: 0.97688 validate_loss: 0.06233 train_acc: 0.75195 validate_acc: 0.75502 \n",
      "Epochs 12 train_loss: 0.92490 validate_loss: 0.04207 train_acc: 0.75781 validate_acc: 0.82932 \n",
      "Epochs 13 train_loss: 0.91446 validate_loss: 0.04534 train_acc: 0.71484 validate_acc: 0.80321 \n",
      "Epochs 14 train_loss: 0.90112 validate_loss: 0.03558 train_acc: 0.76172 validate_acc: 0.85341 \n",
      "Epochs 15 train_loss: 0.84388 validate_loss: 0.04224 train_acc: 0.78320 validate_acc: 0.83333 \n",
      "Epochs 16 train_loss: 0.82473 validate_loss: 0.03245 train_acc: 0.76562 validate_acc: 0.85944 \n",
      "Epochs 17 train_loss: 0.78348 validate_loss: 0.03362 train_acc: 0.77930 validate_acc: 0.86345 \n",
      "Epochs 18 train_loss: 0.75547 validate_loss: 0.02982 train_acc: 0.78906 validate_acc: 0.86948 \n",
      "Epochs 19 train_loss: 0.73286 validate_loss: 0.02527 train_acc: 0.79492 validate_acc: 0.89558 \n",
      "Epochs 20 train_loss: 0.68238 validate_loss: 0.02262 train_acc: 0.80273 validate_acc: 0.89759 \n",
      "Epochs 21 train_loss: 0.67516 validate_loss: 0.02349 train_acc: 0.80469 validate_acc: 0.88956 \n",
      "Epochs 22 train_loss: 0.67134 validate_loss: 0.02237 train_acc: 0.80273 validate_acc: 0.89157 \n",
      "Epochs 23 train_loss: 0.63129 validate_loss: 0.01829 train_acc: 0.80078 validate_acc: 0.90763 \n",
      "Epochs 24 train_loss: 0.60600 validate_loss: 0.01602 train_acc: 0.83594 validate_acc: 0.92570 \n",
      "Epochs 25 train_loss: 0.60279 validate_loss: 0.01481 train_acc: 0.84375 validate_acc: 0.92369 \n",
      "Epochs 26 train_loss: 0.58819 validate_loss: 0.01433 train_acc: 0.80859 validate_acc: 0.93775 \n",
      "Epochs 27 train_loss: 0.55830 validate_loss: 0.01198 train_acc: 0.82227 validate_acc: 0.94177 \n",
      "Epochs 28 train_loss: 0.54391 validate_loss: 0.01183 train_acc: 0.84180 validate_acc: 0.94578 \n",
      "Epochs 29 train_loss: 0.54021 validate_loss: 0.01365 train_acc: 0.85352 validate_acc: 0.93173 \n",
      "Epochs 30 train_loss: 0.52421 validate_loss: 0.01420 train_acc: 0.84180 validate_acc: 0.93574 \n",
      "Epochs 31 train_loss: 0.49306 validate_loss: 0.01169 train_acc: 0.88477 validate_acc: 0.94578 \n",
      "Epochs 32 train_loss: 0.48925 validate_loss: 0.01378 train_acc: 0.85352 validate_acc: 0.94378 \n",
      "Epochs 33 train_loss: 0.49334 validate_loss: 0.01360 train_acc: 0.86328 validate_acc: 0.93574 \n",
      "Epochs 34 train_loss: 0.46166 validate_loss: 0.01086 train_acc: 0.87500 validate_acc: 0.94177 \n",
      "Epochs 35 train_loss: 0.47018 validate_loss: 0.00991 train_acc: 0.86523 validate_acc: 0.95582 \n",
      "Epochs 36 train_loss: 0.45173 validate_loss: 0.01030 train_acc: 0.85156 validate_acc: 0.95181 \n",
      "Epochs 37 train_loss: 0.42581 validate_loss: 0.01102 train_acc: 0.86914 validate_acc: 0.95181 \n",
      "Epochs 38 train_loss: 0.42599 validate_loss: 0.00878 train_acc: 0.89258 validate_acc: 0.95582 \n",
      "Epochs 39 train_loss: 0.43975 validate_loss: 0.01019 train_acc: 0.87109 validate_acc: 0.94980 \n",
      "Epochs 40 train_loss: 0.41852 validate_loss: 0.00959 train_acc: 0.86328 validate_acc: 0.94980 \n",
      "Epochs 41 train_loss: 0.40144 validate_loss: 0.00959 train_acc: 0.87305 validate_acc: 0.95783 \n",
      "Epochs 42 train_loss: 0.42034 validate_loss: 0.00847 train_acc: 0.86719 validate_acc: 0.95181 \n",
      "Epochs 43 train_loss: 0.40596 validate_loss: 0.01207 train_acc: 0.88086 validate_acc: 0.93976 \n",
      "Epochs 44 train_loss: 0.39951 validate_loss: 0.01025 train_acc: 0.86523 validate_acc: 0.94378 \n",
      "Epochs 45 train_loss: 0.35567 validate_loss: 0.00986 train_acc: 0.87500 validate_acc: 0.94378 \n",
      "Epochs 46 train_loss: 0.34160 validate_loss: 0.00721 train_acc: 0.89258 validate_acc: 0.95582 \n",
      "Epochs 47 train_loss: 0.34038 validate_loss: 0.00743 train_acc: 0.88281 validate_acc: 0.95984 \n",
      "Epochs 48 train_loss: 0.33216 validate_loss: 0.00794 train_acc: 0.89844 validate_acc: 0.96185 \n",
      "Epochs 49 train_loss: 0.31802 validate_loss: 0.00763 train_acc: 0.90820 validate_acc: 0.95984 \n",
      "Epochs 50 train_loss: 0.31411 validate_loss: 0.00739 train_acc: 0.88477 validate_acc: 0.95181 \n",
      "ðŸ¤–Training on cuda\n"
     ]
    }
   ],
   "source": [
    "train_da(teacher_model, model, optimizer, criterion, distillation_loss, train_loader, test_loader, test_loader, 50, checkpoint_path=checkpoint_path, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
